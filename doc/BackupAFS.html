<?xml version="1.0" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>BackupAFS</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rev="made" href="mailto:root@localhost" />
</head>

<body id="_podtop_" style="background-color: white">
<table border="0" width="100%" cellspacing="0" cellpadding="3">
<tr><td class="_podblock_" style="background-color: #cccccc" valign="middle">
<big><strong><span class="_podblock_">&nbsp;BackupAFS</span></strong></big>
</td></tr>
</table>



<ul id="index">
  <li><a href="#BackupAFS-Introduction">BackupAFS Introduction</a>
    <ul>
      <li><a href="#Overview">Overview</a></li>
      <li><a href="#Backup-basics">Backup basics</a></li>
      <li><a href="#Resources">Resources</a></li>
      <li><a href="#Road-map">Road map</a></li>
      <li><a href="#You-can-help">You can help</a></li>
    </ul>
  </li>
  <li><a href="#Installing-BackupAFS">Installing BackupAFS</a>
    <ul>
      <li><a href="#Requirements">Requirements</a></li>
      <li><a href="#What-type-of-storage-space-do-I-need">What type of storage space do I need?</a></li>
      <li><a href="#But-how-much-disk-space-do-I-REALLY-need">But how much disk space do I REALLY need?</a></li>
      <li><a href="#Step-1:-Getting-BackupAFS">Step 1: Getting BackupAFS</a></li>
      <li><a href="#Step-2:-Satisfying-the-Dependencies">Step 2: Satisfying the Dependencies</a></li>
      <li><a href="#Step-3:-Installing-the-BackupAFS-software">Step 3: Installing the BackupAFS software</a></li>
      <li><a href="#Step-4:-Setting-up-config.pl">Step 4: Setting up config.pl</a></li>
      <li><a href="#Step-5:-Adding-your-AFS-KeyFile">Step 5: Adding your AFS KeyFile</a></li>
      <li><a href="#Step-6:-Setting-up-the-VolumeSet-List-file">Step 6: Setting up the VolumeSet-List file</a></li>
      <li><a href="#Step-7:-CGI-interface">Step 7: CGI interface</a></li>
      <li><a href="#Step-8:-Running-BackupAFS">Step 8: Running BackupAFS</a></li>
      <li><a href="#Step-9:-Talking-to-BackupAFS">Step 9: Talking to BackupAFS</a></li>
      <li><a href="#Step-10:-Checking-email-delivery">Step 10: Checking email delivery</a></li>
      <li><a href="#Other-installation-topics">Other installation topics</a></li>
      <li><a href="#Fixing-installation-problems">Fixing installation problems</a></li>
    </ul>
  </li>
  <li><a href="#Restore-functions">Restore functions</a>
    <ul>
      <li><a href="#Browser-Download">Browser Download</a></li>
      <li><a href="#Direct-Restore">Direct Restore</a></li>
    </ul>
  </li>
  <li><a href="#Other-CGI-Functions">Other CGI Functions</a>
    <ul>
      <li><a href="#Configuration-and-Host-Editor">Configuration and Host Editor</a></li>
      <li><a href="#RSS">RSS</a></li>
    </ul>
  </li>
  <li><a href="#BackupAFS-Design">BackupAFS Design</a>
    <ul>
      <li><a href="#Some-design-issues">Some design issues</a></li>
      <li><a href="#BackupAFS-operation">BackupAFS operation</a></li>
      <li><a href="#Storage-layout">Storage layout</a></li>
      <li><a href="#File-name-mangling">File name mangling</a></li>
      <li><a href="#Limitations">Limitations</a></li>
      <li><a href="#Security-issues">Security issues</a></li>
    </ul>
  </li>
  <li><a href="#Configuration-File">Configuration File</a>
    <ul>
      <li><a href="#Modifying-the-main-configuration-file">Modifying the main configuration file</a></li>
    </ul>
  </li>
  <li><a href="#Configuration-Parameters">Configuration Parameters</a>
    <ul>
      <li><a href="#General-server-configuration">General server configuration</a></li>
      <li><a href="#What-to-backup-and-when-to-do-it">What to backup and when to do it</a></li>
      <li><a href="#How-to-backup-a-VolumeSet">How to backup a VolumeSet</a></li>
      <li><a href="#Email-reminders-status-and-messages">Email reminders, status and messages</a></li>
      <li><a href="#CGI-user-interface-configuration-settings">CGI user interface configuration settings</a></li>
    </ul>
  </li>
  <li><a href="#Migrating-from-BackupPC4AFS">Migrating from BackupPC4AFS</a>
    <ul>
      <li><a href="#Migrating-VolumeSets">Migrating VolumeSets</a></li>
      <li><a href="#Unmangling-the-Existing-Backups">Unmangling the Existing Backups</a></li>
      <li><a href="#Compression-of-Existing-Backups">Compression of Existing Backups</a></li>
    </ul>
  </li>
  <li><a href="#Version-Numbers">Version Numbers</a></li>
  <li><a href="#Author">Author</a></li>
  <li><a href="#Copyright">Copyright</a></li>
  <li><a href="#Credits">Credits</a></li>
  <li><a href="#License">License</a></li>
</ul>

<a href="#_podtop_"><h1 id="BackupAFS-Introduction">BackupAFS Introduction</h1></a>

<p>This documentation describes BackupAFS version 1.0.0, released on 15 Sep 2015.</p>

<h2 id="Overview">Overview</h2>

<p>BackupAFS is a high-performance, enterprise-grade system for backing up OpenAFS volumes to a server&#39;s disk. This is a strategy commonly known as &quot;disk to disk&quot;, or &quot;disk2disk&quot; backup, and thanks to the low cost and high performance of modern hard disks, is both fast and economical. BackupAFS is highly configurable and easy to install and maintain.</p>

<p>Given the ever decreasing cost of disks and raid systems, it is now practical and cost effective to store backups on a (remote) server&#39;s local disk or network storage. For some sites this might be the complete backup solution. For other sites additional permanent archives could be created by periodically backing up the server to tape.</p>

<p>Features include:</p>

<ul>

<li><p>Optional compression provides additional reductions in storage (around 35-40%, depending on your data).</p>

</li>
<li><p>A powerful http/cgi user interface allows administrators to view the current status, edit configuration, add/delete volumesets, view log files, and allows users to initiate and cancel backups and browse and restore volumes from backups.</p>

</li>
<li><p>No additional software is needed. BackupAFS uses the OpenAFS &#39;vos&#39; binary and stores its files in standard &#39;vos dump&#39; format.</p>

</li>
<li><p>Flexible restore options. Volume dump files can be downloaded from any backup directly from the CGI interface. Volumes may also be restored directly into AFS, overwriting any existing volume of the same name or with an optional extension (.restore for instance) to prevent namespace collisions.</p>

</li>
<li><p>Flexible configuration parameters allow a configurable number of multiple backups to be performed in parallel, specification of which volumes to backup, various schedules for full and incremental backups, schedules for email notifications to users and admins and so on. Configuration parameters can be set system-wide or also on a per-volumeset basis.</p>

</li>
<li><p>Admins (and optionally users) may be sent periodic email reminders if their volumeset has not recently been backed up. Email content, timing and policies are configurable.</p>

</li>
<li><p>BackupAFS is Open Source software hosted by SourceForge.</p>

</li>
</ul>

<h2 id="Backup-basics">Backup basics</h2>

<dl>

<dt id="Full-Backup">Full Backup</dt>
<dd>

<p>A full backup is a complete record of an object (file, volume, volumeset, etc). BackupAFS operates on <b>volume sets</b>, which are collections of one or more volumes with common characteristics (generally the same location and/or names that match specified patterns). BackupAFS can be configured to do a full backup at a regular interval (typically monthly). BackupAFS can be configured to keep a certain number of full backups. Exponential expiry is also supported, allowing full backups with various vintages to be kept (for example, a specified number of most recent monthly fulls, plus a specified number of older fulls that are 2, 4, 8, or 16 months apart).</p>

</dd>
<dt id="Incremental-Backup">Incremental Backup</dt>
<dd>

<p>An incremental backup is a record of changes to one or more objects (files, volumes, volumeset, etc.) that have changed since the last successful full or lower-leveled incremental backup. BackupAFS operates on <b>volume sets</b>, which are collections of one or more volumes with common characteristics (generally the same location and/or names that match specified patterns). Multi-level incrementals are supported. A full backup has level 0. A new incremental of level N will backup all files in a given volume that have changed since the most recent backup of a lower level. <a href="#_conf_incrlevels_">$Conf{IncrLevels}</a> is used to specify the level of each successive incremental. The default value is all level 1, which makes the behavior the same as earlier versions of BackupAFS: each incremental will back up all the files that changed since the last full (level 0).</p>

<p>Immediately before a &quot;vos dump&quot; is performed, the volume in question has a &quot;vos backup&quot; performed on it.</p>

<p>Vos backups are performed using the &quot;vos dump -time &lt;dump from time&gt;&quot; syntax. Discussion on the OpenAFS mailing list confirmed that the -time argument catches even new files with old timestamps (created via touch, tar, etc.) because during vos dumps, AFS decides whether a file has been changed based on the vnode:uniquifier data.</p>

<p>BackupAFS can also be configured to keep a certain number of incremental backups, and to keep a smaller number of very old incremental backups. If multi-level incrementals are specified then it is likely that more incrementals will be kept than specified, since lower-level incrementals (and the full backup) are needed to reconstruct a higher-level incremental.</p>

<p>BackupAFS &quot;fills-in&quot; incremental backups when browsing or restoring, based on the levels of each backup, giving every backup a &quot;full&quot; appearance. This makes browsing and restoring backups much easier: you can restore from any one backup independent of whether it was an incremental or full. BackupAFS will construct the dependency list and restore all parent dumps necessary to reconstruct the original volume.</p>

</dd>
<dt id="Backup-Policy">Backup Policy</dt>
<dd>

<p>Based on your site&#39;s requirements you need to decide what your backup policy is. BackupAFS is not designed to provide exact re-imaging of an AFS cell. See <a>Limitations</a> for more information. However, an exact restoration of individual or multiple volumes may be made.</p>

<p>BackupAFS saves backups onto disk. Because of compression you can economically keep several weeks or months of old backups.</p>

<p>At some sites the disk-based backup will be adequate without a secondary tape backup. This system is robust to any single failure: if an AFS fileserver fails or loses volumes, the BackupAFS server can be used to restore those volumes. If the BackupAFS server disk fails, BackupAFS can be restarted on a fresh file system, and create new backups from the volumes still in AFS. The chance of the server disk failing can be made very small by spending more money on increasingly better RAID systems. However, there is still the risk of catastrophic events like fires or earthquakes that can destroy both the BackupAFS server and the fileservers it is backing up if they are physically nearby. Physically separating the BackupAFS server from the AFS fileservers is recommended.</p>

<p>Some sites might choose to do periodic backups to tape or cd/dvd in parallel with BackupAFS. Another alternative would be to do tape backups of the BackupAFS data partition (or only when BackupAFS performs a full dumps, etc.).</p>

<p>Other users have reported success with removable disks to rotate the BackupAFS data drives, or using rsync to mirror the BackupAFS data pool offsite. The truely paranoid might run multiple BackupAFS instances in parallel on separate hardware, at separate locations, or run BackupAFS in parallel with another AFS-aware backup product.</p>

</dd>
</dl>

<h2 id="Resources">Resources</h2>

<dl>

<dt id="BackupAFS-home-page">BackupAFS home page</dt>
<dd>

<p>The BackupAFS Open Source project is hosted on SourceForge. The home page can be found at:</p>

<pre><code>    http://backupafs.sourceforge.net</code></pre>

<p>This page has links to the current documentation, the SourceForge project page and general information.</p>

</dd>
<dt id="SourceForge-project">SourceForge project</dt>
<dd>

<p>The SourceForge project page is at:</p>

<pre><code>    http://sourceforge.net/projects/backupafs</code></pre>

<p>This page has links to the current releases of BackupAFS.</p>

</dd>
<dt id="Other-Programs-of-Interest">Other Programs of Interest</dt>
<dd>

<p>If you just want to mirror linux or unix files or directories that happen to reside in AFS to a remote server you could use rsync, <a href="http://rsync.samba.org">http://rsync.samba.org</a>. You may wish to use rsync to mirror the BackupAFS data store. The BackupAFS data store could also be replicated across the network in realtime with DRBD (for that matter, so could your AFS servers).</p>

<p>Two popular open source packages that do tape backup are Amanda (<a href="http://www.amanda.org">http://www.amanda.org</a>) and Bacula (<a href="http://www.bacula.org">http://www.bacula.org</a>). There are AFS extensions for Amanda (see Amanda-afs below), therefore it might be used as a complete solution. Further either might be used as a back end to BackupAFS to dump the BackupAFS server data to tape.</p>

<p>AFS native backup (butc) - designed to write directly to tapes, but can be configured to write to files directly. Notoriously difficult to configure, manage, and interface with tape libraries and autochangers. When dumping to files, one file represents an entire volumeset (this is different from &quot;vos&quot; dumps, where one file represents one volume).</p>

<p>AFS &quot;vos dump&quot; &amp; &quot;vos restore&quot; - basic AFS commands to dump and restore AFS volumes to and from files. This is what BackupPC4AFS does internally, and many sites use &quot;homegrown&quot; scripts which rely on vos internally.</p>

<p>Amanda-afs - AFS extensions to allow Amanda to backup and restore AFS files. Appears to do this via a vos &lt;-&gt; tar translator.</p>

<p>TSM - Commercial software. There is an extension to IBM&#39;s Tivoli Storage Manager to allow it to backup AFS volumes. Apparently no longer marketed or supported.</p>

<p>Veritas NetBackup AFS module - Commercial software. Also apparently no longer supported. Anyone with additional information, please feel free to forward it.</p>

<p>TiBS, Teradactyl&#39;s &quot;True incremental Backup System&quot; - Closed-source, commercial software. Actively marketed and supported. While actual prices are not reported on Teradactyl&#39;s website, prices charged sites seem to vary somewhat but are usually quite expensive. Teradactyl is an active OpenAFS supporter and several contributors to the OpenAFS lists have admitted to using it.</p>

<p>Others?</p>

<p>BackupAFS provides many additional features, such as compressed storage, easy configuration via CGI, etc. But these other programs provide simple, effective and fast solutions and are definitely worthy of consideration, dependent on your needs and budget.</p>

</dd>
</dl>

<h2 id="Road-map">Road map</h2>

<p>The primary developer has some ideas for new features for future releases of BackupAFS. Comments and suggestions are welcome.</p>

<h2 id="You-can-help">You can help</h2>

<p>BackupAFS is free. I work on BackupAFS because it satisfies the requirements of my use, because I enjoy doing it, and because I like to contribute to the open source community.</p>

<p>My main compensation for continuing to work on BackupAFS is knowing that more and more people find it useful. So feedback is certainly appreciated, both positive and negative.</p>

<p>Beyond being a satisfied user and telling other people about it, everyone is encouraged to add links to <a href="http://backupafs.sourceforge.net">http://backupafs.sourceforge.net</a> (I&#39;ll see them via Google) or otherwise publicize BackupAFS. Unlike the commercial products in this space, I have a zero budget (in both time and money) for marketing, PR and advertising, so it&#39;s up to all of you! Feel free to vote for BackupAFS at <a href="http://freshmeat.net/projects/backupafs">http://freshmeat.net/projects/backupafs</a>.</p>

<p>Also, everyone is encouraged to contribute patches, bug reports, feature and design suggestions, new code, and documentation corrections or improvements. Answering questions on the mailing list is a big help too.</p>

<a href="#_podtop_"><h1 id="Installing-BackupAFS">Installing BackupAFS</h1></a>

<h2 id="Requirements">Requirements</h2>

<p>BackupAFS requires:</p>

<ul>

<li><p>Linux (although Solaris, MacOS X, or other unix-based operating systems can likely be made to work with very little effort).</p>

</li>
<li><p>A substantial amount of free disk space (see the next section for what that means). The CPU and disk performance on this server will determine how many simultaneous backups you can run. You should be able to run 4-8 simultaneous backups on a moderately configured server.</p>

<p>Several sites have reported significantly better performance using xfs compared to ext3 for the BackupAFS data filesystem. It is also recommended you consider either an LVM or RAID setup (either in HW or SW; eg: 3Ware RAID10, RAID5, or RAID6) so that you can expand the file system as necessary.</p>

</li>
<li><p>The standard OpenAFS client, including the &#39;vos&#39; binary. Use the version provided by your OS, or get it from <a href="http://www.openafs.org">http://www.openafs.org</a>.</p>

</li>
<li><p>Perl version 5.8.0 or later. If you don&#39;t have perl, please see <a href="http://www.cpan.org">http://www.cpan.org</a>.</p>

</li>
<li><p>Perl modules Compress::Zlib (optional) for compressing log files. Try &quot;perldoc Compress::Zlib&quot; to see if you have this module. If not, fetch it from <a href="http://www.cpan.org">http://www.cpan.org</a> and see the instructions below for how to build and install them.</p>

</li>
<li><p>The gzip or pigz binary (optional) for compressing dumps. <b>For multi-core systems, pigz (parallelized gzip algorithm) is strongly suggested</b>. If your OS lacks these, they can be obtained from <a href="http://www.gzip.org">http://www.gzip.org</a> and <a href="http://www.zlib.net/pigz">http://www.zlib.net/pigz</a>.</p>

</li>
<li><p>The Apache web server, see <a href="http://www.apache.org">http://www.apache.org</a>, preferably built with mod_perl support.</p>

</li>
</ul>

<h2 id="What-type-of-storage-space-do-I-need">What type of storage space do I need?</h2>

<p>BackupAFS stores its dumps of volumesets on disk. Therefore BackupAFS&#39;s data store (__TOPDIR__) should point to a single, large file system (it is ok to use a single symbolic link at the top-level directory (__TOPDIR__) to point the entire data store somewhere else). You can of course use any kind of RAID system or logical volume manager that combines the capacity of multiple disks into a single, larger, file system. Such approaches have the advantage that the file system can be expanded without having to copy it.</p>

<p>Any standard linux or unix file system should work fine. NFS mounted file systems work too. Windows based FAT and NTFS file systems have not been tested, but should work in principle (BackupAFS does not use hard links, but some paths may get very long, depending on both site data store location and volume- and volumeset-naming decisions.</p>

<h2 id="But-how-much-disk-space-do-I-REALLY-need">But how much disk space do I REALLY need?</h2>

<p>Here&#39;s one real example for an environment that is backing up 95 volumesets (3500 total volumes) with compression off. The full AFS cell is ~3.5 TB. Storing two full bi-monthly backups and 60 incremental backups per volumeset is around 7.3TB of raw data.</p>

<p>The same cell, with compression on: backing up 95 volumesets (3500 total volumes). Storing two full bi-monthly backups and 60 incremental backups per volumeset takes only 4.7TB of space (approx. a 36% savings).</p>

<p>Your actual mileage will depend upon the types and compressibility of data you backup. The more compressible the data, the bigger benefit compression will be to you.</p>

<h2 id="Step-1:-Getting-BackupAFS">Step 1: Getting BackupAFS</h2>

<p>Manually fetching and installing BackupAFS is easy. Start by downloading the latest version from <a href="http://backupafs.sourceforge.net">http://backupafs.sourceforge.net</a>. Hit the &quot;Code&quot; button, then select the &quot;backupafs&quot; or &quot;backupafs-beta&quot; package and download the latest version.</p>

<h2 id="Step-2:-Satisfying-the-Dependencies">Step 2: Satisfying the Dependencies</h2>

<p>Note: most information in this step is only relevant if you build and install BackupAFS yourself. If you use a package provided by a distribution, the package management system should take of installing any needed dependencies.</p>

<p>First off, there are two perl modules you should install. These are all optional, but highly recommended:</p>

<dl>

<dt id="Compress::Zlib">Compress::Zlib</dt>
<dd>

<p>To enable log compression, you will need to install Compress::Zlib from <a href="http://www.cpan.org">http://www.cpan.org</a>. You can run &quot;perldoc Compress::Zlib&quot; to see if this module is installed.</p>

</dd>
<dt id="XML::RSS">XML::RSS</dt>
<dd>

<p>To support the (experimental) RSS feature you will need to install XML::RSS, also from <a href="http://www.cpan.org">http://www.cpan.org</a>. There is not need to install this module if you don&#39;t plan on using RSS. You can run &quot;perldoc XML::RSS&quot; to see if this module is installed.</p>

<p>To build and install these packages you may wish to use the cpan program. Alternatively, you can fetch the tar.gz file from <a href="http://www.cpan.org">http://www.cpan.org</a> and then run these commands:</p>

<pre><code>    tar zxvf Compress-Zip-1.26.tar.gz
    cd Compress-Zip-1.26
    perl Makefile.PL
    make
    make test
    make install</code></pre>

<p>The same sequence of commands can be used for each module.</p>

</dd>
<dt id="OpenAFS-Client">OpenAFS Client</dt>
<dd>

<p>The BackupAFS server must be an AFS client for your AFS cell. So, if you have not already, you should install and configure the OpenAFS client using the settings for your cell. Many popular linux distributions include a pre-packaged version of the client. If your distribution does not, or if you prefer not to use it, you can download it from <a href="http://www.openafs.org">http://www.openafs.org</a>.</p>

</dd>
<dt id="Compression-executable">Compression executable</dt>
<dd>

<p>BackupAFS can optionally compress volume dumps using either gzip or pigz for compression. Both applications perform compression using the gzip algorithm; pigz is simply a parallel implementation designed to leverage multiple cores in modern multi-CPU servers. If your distribution does not include the one you choose, you may download Gzip from <a href="http://www.gzip.org/">http://www.gzip.org/</a> or Pigz from <a href="http://www.zlib.net/pigz/">http://www.zlib.net/pigz/</a>. <b>If you have a multi-processor system, pigz is strongly recommended.</b></p>

</dd>
</dl>

<h2 id="Step-3:-Installing-the-BackupAFS-software">Step 3: Installing the BackupAFS software</h2>

<p>Now let&#39;s move onto BackupAFS itself. After fetching BackupAFS-1.0.0.tar.gz, run these commands as root:</p>

<pre><code>    tar zxf BackupAFS-1.0.0.tar.gz
    cd BackupAFS-1.0.0
    perl configure.pl</code></pre>

<p>In the future this release might also have patches available on the SourceForge site. These patch files are text files, with a name of the form</p>

<pre><code>    BackupAFS-1.0.0plN.diff</code></pre>

<p>where N is the patch level, eg: pl2 is patch-level 2. These patch files are cumulative: you only need apply the last patch file, not all the earlier patch files. If a patch file is available, eg: BackupAFS-1.0.0pl2.diff, you should apply the patch after extracting the tar file:</p>

<pre><code>     # fetch BackupAFS-1.0.0.tar.gz
     # fetch BackupAFS-1.0.0pl2.diff
     tar zxf BackupAFS-1.0.0.tar.gz
     cd BackupAFS-1.0.0
     patch -p0 &lt; ../BackupAFS-1.0.0pl2.diff
     perl configure.pl</code></pre>

<p>A patch file includes comments that describe that bug fixes and changes. Feel free to review it before you apply the patch.</p>

<p>The configure.pl script also accepts command-line options if you wish to run it in a non-interactive manner. It has self-contained documentation for all the command-line options, which you can read with perldoc:</p>

<pre><code>    perldoc configure.pl</code></pre>

<p>The configure.pl script by default complies with the file system hierarchy (FHS) conventions. The configuration files will be stored in /etc/BackupAFS and the log files will be stored in /var/log/BackupAFS.</p>

<p>Note that distributions may choose to use different locations for BackupAFS files than these defaults.</p>

<p>If you are upgrading from an earlier version the configure.pl script will keep the configuration files and log files in their original location.</p>

<p>When you run configure.pl you will be prompted for the full paths of various executables, and you will be prompted for the following information.</p>

<dl>

<dt id="BackupAFS-User">BackupAFS User</dt>
<dd>

<p>It is best if BackupAFS runs as a special user, eg backupafs, that has limited privileges. It is preferred that backupafs belongs to a system administrator group so that sys admin members can browse BackupAFS files, edit the configuration files and so on. Although configurable, the default settings leave group read permission on backup directories, so make sure the BackupAFS user&#39;s group is chosen restrictively.</p>

<p>On this installation, this is backup.</p>

<p>For security purposes you might choose to configure the BackupAFS user with the shell set to /bin/false. Since you might need to run some BackupAFS programs as the BackupAFS user for testing purposes, you can use the -s option to su to explicitly run a shell, eg:</p>

<pre><code>    su -s /bin/bash backup</code></pre>

<p>Depending upon your configuration you might also need the -l option.</p>

</dd>
<dt id="Data-Directory">Data Directory</dt>
<dd>

<p>You need to decide where to put the data directory, below which all the BackupAFS data is stored. This needs to be a big file system. Unless your largest volumes are tiny, it needs to support largefiles (&gt;2GB). You may wish to consider xfs or zfs (if available for your OS).</p>

<p>On this installation, this is __TOPDIR__.</p>

</dd>
<dt id="Install-Directory">Install Directory</dt>
<dd>

<p>You should decide where the BackupAFS scripts, libraries and documentation should be installed, eg: __INSTALLDIR__.</p>

<p>On this installation, this is __INSTALLDIR__.</p>

</dd>
<dt id="CGI-bin-Directory">CGI bin Directory</dt>
<dd>

<p>You should decide where the BackupAFS CGI script resides. This will usually be below Apache&#39;s cgi-bin directory.</p>

<p>It is also possible to use a different directory and use Apache&#39;s &quot;&lt;Directory&gt;&quot; directive to specifiy that location. See the Apache HTTP Server documentation for additional information.</p>

<p>On this installation, this is /usr/lib/cgi-bin.</p>

</dd>
<dt id="Apache-image-Directory">Apache image Directory</dt>
<dd>

<p>A directory where BackupAFS&#39;s images are stored so that Apache can serve them. You should ensure this directory is readable by Apache and create a symlink to this directory from the BackupAFS CGI bin Directory.</p>

</dd>
<dt id="Config-and-Log-Directories">Config and Log Directories</dt>
<dd>

<p>In this installation the configuration and log directories are located in the following locations:</p>

<pre><code>    /etc/BackupAFS/config.pl            main config file
    /etc/BackupAFS/VolumeSet-List       file with list and defs of volume sets
    /etc/BackupAFS/volsets/VOLUMESET.pl per-pc config file
    /var/log/BackupAFS/BackupAFS        log files, pid, status</code></pre>

<p>The configure.pl script doesn&#39;t prompt for these locations but they can be set for new installations using command-line options.</p>

</dd>
</dl>

<p>An example of this step:</p>

<pre><code>    mkdir __INSTALLDIR__-1.0.0
    cd /tmp
    cp /path/to/BackupAFS-1.0.0.tar.gz ./
    tar -zxvpf BackupAFS-1.0.0.tar.gz
    cd BackupAFS-1.0.0
    perl configure.pl
    --&gt; Full path to existing main config.pl []?
    --&gt; Are these paths correct? [y]? y
    --&gt; BackupAFS will run on host [backupserver]? backupserver
    --&gt; BackupAFS should run as user [backupafs]? backup
    --&gt; Install directory (full path) [__INSTALLDIR__]? __INSTALLDIR__
    --&gt; Data directory (full path) [__TOPDIR__]? __TOPDIR__
    --&gt; Compression level [0]? 4
    --&gt; CGI bin directory (full path) []? /usr/lib/cgi-bin
    --&gt; Apache image directory (full path) []? /var/www/BackupAFS
    --&gt; URL for image directory (omit http://host; starts with &#39;/&#39;) []? /BackupAFS
    --&gt; Do you want to continue? [y]?</code></pre>

<h2 id="Step-4:-Setting-up-config.pl">Step 4: Setting up config.pl</h2>

<p>After running configure.pl, browse through the config file, /etc/BackupAFS/config.pl, and make sure all the default settings are correct. In particular, you will need to configure one or more CGI admin users or groups in order to perform useful tasks via the CGI.</p>

<p>If you&#39;re using mod_auth_kerb, make sure that you specify the CgiAdminUsers in a username@KRB5.SOME.DOMAIN.ORG format.</p>

<p>Example:</p>

<pre><code>    grep CgiAdminUsers /etc/BackupAFS/config.pl | grep -v ^#
    <a href="#_conf_cgiadminusers_">$Conf{CgiAdminUsers}</a> = &#39;admin@K5REALM.UNIV.EDU&#39;;</code></pre>

<p>or:</p>

<pre><code>    <a href="#_conf_cgiadminusers_">$Conf{CgiAdminUsers}</a> = &#39;admin1@K5REALM.UNIV.EDU,admin2@K5REALM.UNIV.EDU&#39;;</code></pre>

<h2 id="Step-5:-Adding-your-AFS-KeyFile">Step 5: Adding your AFS KeyFile</h2>

<p>In order to perform vos dump operations without tokens, we need to copy the AFS cell&#39;s keyfile to the BackupAFS server. <b>This keyfile must be protected</b>. Only the backupafs user should be able to read it. <i>This means that the BackupAFS server should be kept as secure as your AFS fileservers.</i> This document won&#39;t devolve into a security lecture, but suffice it to say you should turn off all unnecessary services and wrap or firewall the remaining services.</p>

<p>Modern Debian/Ubuntu distributions store the KeyFile in the /etc/openafs/server directory. That directory on AFS fileserver machines normally includes other files (ThisCell, CellServDB, UserList, and possibly CellAlias); those files will not harm the BackupAFS server. It is advised to copy the entire /etc/openafs/server directory from an existing fileserver to the BackupAFS server.</p>

<p>If the version of OpenAFS supplied by your operating system (or you, if you manually install) expects the KeyFile in a different location, place the copy in that location instead of /etc/openafs/server.</p>

<p>Example:</p>

<pre><code>    backupserver:~ # mkdir -p /etc/openafs/server
    backupserver:~ # chmod 700 /etc/openafs/server
    backupserver:~ # cd /etc/openafs
    backupserver:/etc/openafs # scp -R root@fileserver1:/etc/openafs/server server/
    root@fileserver1&#39;s password:
    KeyFile                                       100%   16     0.0KB/s   00:00    
    ThisCell                                      100%   16     0.0KB/s   00:00    
    CellServDB                                    100%   16     0.0KB/s   00:00    
    UserList                                      100%   16     0.0KB/s   00:00    
    backupserver:/etc/openafs/server # chown -R backupafs:backupafs.
    backupserver:/etc/openafs/server # chmod 700 KeyFile</code></pre>

<h2 id="Step-6:-Setting-up-the-VolumeSet-List-file">Step 6: Setting up the VolumeSet-List file</h2>

<p>The file /etc/BackupAFS/VolumeSet-List contains the list and definitions of volumesets to backup. BackupAFS reads this file in three cases:</p>

<ul>

<li><p>Upon startup.</p>

</li>
<li><p>When BackupAFS is sent a HUP (-1) signal. Assuming you installed the init.d script, you can also do this with &quot;/etc/init.d/backupafs reload&quot;.</p>

</li>
<li><p>When the modification time of the VolumeSet-List file changes. BackupAFS checks the modification time once during each regular wakeup.</p>

</li>
</ul>

<p>Whenever you change the VolumeSet-List file (to add or remove a volset) you can either do a <code>kill -HUP <i>BackupAFS_pid</i></code> or simply wait until the next regular wakeup period.</p>

<p>Each line in the VolumeSet-List file contains multiple fields, separated by a colon (:)</p>

<dl>

<dt id="volset-name-of-the-VolumeSet">volset (name of the VolumeSet)</dt>
<dd>

<p>This is the definitive name of the volumeset and should be in lower case. The volset name should contain only lowercase alphanumerics. The use of non-alphanumeric characters may be possible (preceded by a backslash), but it is not recommended.</p>

</dd>
<dt id="user-user-name">user (user name)</dt>
<dd>

<p>This should be the unix login/email name of the user who &quot;owns&quot; or uses this volset. This is the user who will be sent email about this volset, and this user will have permission to stop/start/browse/restore backups for this volset. Leave this blank if no specific person should receive email or be allowed to stop/start/browse/restore backups for this volset. Administrators will still have full permissions.</p>

</dd>
<dt id="moreUsers-more-user-names">moreUsers (more user names)</dt>
<dd>

<p>Additional user names, separated by commas and with no white space, can be specified. These users will also have full permission in the CGI interface to stop/start/browse/restore backups for this volset. These users will not be sent email about this volset.</p>

</dd>
<dt id="volume-entries-1---5">(volume entries 1 - 5)</dt>
<dd>

<p>To paraphrase the OpenAFS Administrator&#39;s Guide, &quot;a single volumeset consists of [between one and five] volume entries, each of which specifies which volumes to backup based on their location (file server machine and partition) and volume name.&quot; BackupAFS represents the location as Entry<i>X</i>_Servers and Entry<i>X</i>_Partitions. Volume names are specified by Entry<i>X</i>_Volumes, where X is an integer between 1 and 5, inclusive.</p>

<p>The full list of fields which represent volume entries is</p>

<pre><code>    Entry1_Servers, Entry1_Partitions, Entry1_Volumes
    Entry2_Servers, Entry2_Partitions, Entry2_Volumes
    Entry3_Servers, Entry3_Partitions, Entry3_Volumes
    Entry4_Servers, Entry4_Partitions, Entry4_Volumes
    Entry5_Servers, Entry5_Partitions, Entry5_Volumes</code></pre>

<p>All of these fields are actually regular expressions. Those of you familar with regular expressions will recall that the period &quot;.&quot; represents any valid character, therefore literal periods in hostnames must be escaped (preceded by a backslash &quot;\&quot;). <b>Because it is normal to dump only AFS backup volumes (that is the normal volume name with a .backup suffix), it is normal to include \.backup in the regular expression for Entry<i>X</i>_Volumes.</b> BackupAFS will create the .backup volumes if they do not exist (or re-create them if they do), immediately prior to dumping each volume.</p>

<p>Examples of valid values of Entry1_Servers include:</p>

<pre><code>    fileserver1\.mydomain\.com
    london-fs.*\.mydomain\.com
    .*-fs1\.mydomain\.com
    .*</code></pre>

<p>Examples of valid values of Entry<i>X</i>_Partitions include:</p>

<pre><code>    /vicepa
    /vicepa.*
    .*</code></pre>

<p>Examples of valid values of Entry<i>X</i>_Volumes include:</p>

<pre><code>    .*\.backup
    prj\.dept\.backup
    prj\.dept\..*\.backup
    user\.a.*\.backup</code></pre>

<p>Entry1_Servers, Entry1_Partitions, and Entry1_Volumes together define one group of volumes which will be backed up in the volumeset. Each volumeset may have up to 5 volume entries comprising it.</p>

<p>Combining servers, partitions, and volumes usefully in combinations depends on the desired result and the characteristics of the volumes you wish to backup.</p>

<p>Some examples from the OpenAFS Administrator&#39;s Guide:</p>

<ul>

<li><p>To backup all volumes in the VLDB, use the regular expression .* in all three parts of the volume entry (Entry1_Servers, Entry1_Partitions, Entry1_Volumes) (Example &quot;myvolset:::.*:.*:.*\.backup&quot;)</p>

</li>
<li><p>Every volume on a specific fileserver machine, specify its fully qualified hostname in the &quot;Entry<i>X</i>_Servers&quot; field and use the regular expression .* for the Entry<i>X</i>_Partitions and Entry<i>X</i>_Volumes fields. (Example &quot;myvolset:jondoe:janedoe:fileserver1\.mydomain\.com:.*:.*&quot;)</p>

</li>
<li><p>All volumes that reside on a partition with the same name on various file server machines, specify the complete partition name in the Entry<i>X</i>_Partitions field and use the regular expression .* for the Entry<i>X</i>_Servers and Entry<i>X</i>_Volumes fields (Example &quot;myvolset:jondoe:janedoe:.*:/vicepa:.*&quot;)</p>

</li>
<li><p>Every volume with a common string in its name, use the regular expression .* for the Entry<i>X</i>_Servers and Entry<i>X</i>_Partitions fields, and provide a combination of alphanumeric characters and metacharacters in the Entry<i>X</i>_Volumes field (Example &quot;myvolset:jondoe:janedoe:.*:.*:user.*\.backup&quot;)</p>

</li>
<li><p>All volumes on one partition, specify the fileserver machine&#39;s fully qualified hostname as the Entry<i>X</i>_Servers field, the full partition name as the Entry<i>X</i>_Partitions field, and the regular expression .* for the Entry<i>X</i>_Volumes field. (Example &quot;myvolset:jondoe:janedoe:fileserver1\.mydomain\.com:/vicepa:.*\.backup&quot;)</p>

</li>
<li><p>A single volume, specify its complete name in the Entry<i>X</i>_Volumes field. To bypass the potentially time-consuming search through the VLDB for matching entries, you may specify the actual fileserver machine and partition name in the Entry<i>X</i>_Servers and Entry<i>X</i>_Partitions fields, respectively. However if it is possible that you will need to move the volume in the future, it is best to use the regular expression .* for the server and partition names. (Example &quot;myvolset:jondoe:janedow:.*:.*:user\.bigwig\.backup&quot;)</p>

</li>
</ul>

<p>Because all of the volumes within a single volumeset are dumped to disk at the same time (daily, weekly, monthly, etc) and in the same manner (full or incremental), a volumeset generally includes volumes with similar contents or characteristics (as indicated by similar names). This grouping by name is generally more useful than one that treats volumes by location, unless your cell includes geographically-separated fileservers. The most common, most useful value for Entry<i>X</i>_Servers and Entry<i>X</i>_Partitions is the regular expression .* (period followed by an asterisk).</p>

<p>It is generally advisable to include a limited number of volumes in a volume entry. Dumps of a volumeset that includes a large number of volumes can take a long time to complete.</p>

<p>It is generally advisable to strive for staggered full backups. That is if you have 14 volumesets and you intend to perform full backups every 2 weeks, you stagger your backup rotation so that each volumeset&#39;s full takes place on a different night. If you do not do this, or if one volumeset is disproportionately large compared to the others, your BackupAFS server will work hard during that full and be mostly-idle the rest of the time.</p>

</dd>
</dl>

<p>The first non-comment line of the VolumeSet-List file is special: it contains the names of the fields and should not be edited.</p>

<p>Here&#39;s a simple example of a VolumeSet-List file:</p>

<pre><code>    volset:user:moreUsers:Entry1_Servers:Entry1_Partitions:Entry1_Volumes:Entry2_Servers:Entry2_Partitions:Entry2_Volumes:Entry3_Servers:Entry3_Partitions:Entry3_Volumes:Entry4_Servers:Entry4_Partitions:Entry4_Volumes:Entry5_Servers:Entry5_Partitions:Entry5_Volumes
    class_all:jondoe::.*:.*:class\..*\.backup::::::::::::
    user_a:::.*:.*:user\.a.*\.backup::::::::::::
    user_z:::.*:.*:user\.z.*\.backup::::::::::::</code></pre>

<p><b>The easiest method of editing volumesets is to use the CGI, once configured.</b></p>

<p>All fields except volset, Entry1_Servers, Entry1_Partitions, and Entry1_Volumes may be blank.</p>

<h2 id="Step-7:-CGI-interface">Step 7: CGI interface</h2>

<p>The CGI interface script, <code>BackupAFS_Admin</code>, is a powerful and flexible way to see and control what BackupAFS is doing. It is written for an Apache server. If you don&#39;t have Apache, see <a href="http://www.apache.org">http://www.apache.org</a>.</p>

<p>There are two options for setting up the CGI interface: standard mode and using mod_perl. Mod_perl provides much higher performance (around 15x) and is the best choice if your Apache was built with mod_perl support. To see if your apache was built with mod_perl run this command:</p>

<pre><code>    httpd -l | egrep mod_perl</code></pre>

<p>If this prints mod_perl.c then your Apache supports mod_perl.</p>

<p>Note: on some distributions (like Debian) the command is not &quot;httpd&quot;, but &quot;apache&quot; or &quot;apache2&quot;. Those distributions will generally also use &quot;www-data&quot; for the Apache user account and configuration files.</p>

<p>Using mod_perl with BackupAFS_Admin requires a dedicated Apache to be run as the BackupAFS user (backup). This is because BackupAFS_Admin needs permission to access various files in BackupAFS&#39;s data directories. In contrast, the standard installation (without mod_perl) solves this problem by having BackupAFS_Admin installed as setuid to the BackupAFS user, so that BackupAFS_Admin runs as the BackupAFS user.</p>

<p>Here are some specifics for each setup:</p>

<dl>

<dt id="Standard-Setup">Standard Setup</dt>
<dd>

<p>The CGI interface should have been installed by the configure.pl script in /usr/lib/cgi-bin/BackupAFS_Admin. BackupAFS_Admin should have been installed as setuid to the BackupAFS user (backup), in addition to user and group execute permission.</p>

<p>You should be very careful about permissions on BackupAFS_Admin and the directory /usr/lib/cgi-bin: it is important that normal users cannot directly execute or change BackupAFS_Admin, otherwise they can access backup files for any volset. You might need to change the group ownership of BackupAFS_Admin to a group that Apache belongs to so that Apache can execute it (don&#39;t add &quot;other&quot; execute permission!). The permissions should look like this:</p>

<pre><code>    ls -l /usr/lib/cgi-bin/BackupAFS_Admin
    -swxr-x---    1 backup   web      82406 Jun 17 22:58 /usr/lib/cgi-bin/BackupAFS_Admin</code></pre>

<p>The setuid script won&#39;t work unless perl on your machine was installed with setuid emulation. This is likely the problem if you get an error saying such as &quot;Wrong user: my userid is 25, instead of 150&quot;, meaning the script is running as the httpd user, not the BackupAFS user. This is because setuid scripts are disabled by the kernel in most flavors of unix and linux.</p>

<p>To see if your perl has setuid emulation, see if there is a program called sperl5.8.0 (or sperl5.8.2 etc, based on your perl version) in the place where perl is installed. If you can&#39;t find this program, then you have three options: rebuild and reinstall perl with the setuid emulation turned on (answer &quot;y&quot; to the question &quot;Do you want to do setuid/setgid emulation?&quot; when you run perl&#39;s configure script), switch to the mod_perl alternative for the CGI script (which doesn&#39;t need setuid to work), or run apache as the BackupAFS user (which will not require setuid to work)..</p>

</dd>
<dt id="Mod_perl-Setup">Mod_perl Setup</dt>
<dd>

<p>The advantage of the mod_perl setup is that no setuid script is needed, and there is a huge performance advantage. Not only does all the perl code need to be parsed just once, the config.pl and VolumeSet-List files, plus the connection to the BackupAFS server are cached between requests. The typical speedup is around 15 times.</p>

<p>To use mod_perl you need to run Apache as user backup. If you need to run multiple Apache&#39;s for different services then you need to create multiple top-level Apache directories, each with their own config file. You can make copies of /etc/init.d/httpd and use the -d option to httpd to point each http to a different top-level directory. Or you can use the -f option to explicitly point to the config file. Multiple Apache&#39;s will run on different Ports (eg: 80 is standard, 8080 is a typical alternative port accessed via http://yourhost.com:8080).</p>

<p>Inside BackupAFS&#39;s Apache http.conf file you should check the settings for ServerRoot, DocumentRoot, User, Group, and Port. See <a href="http://httpd.apache.org/docs/server-wide.html">http://httpd.apache.org/docs/server-wide.html</a> for more details.</p>

<p>For mod_perl, BackupAFS_Admin should not have setuid permission, so you should turn it off:</p>

<pre><code>    chmod u-s /usr/lib/cgi-bin/BackupAFS_Admin</code></pre>

<p>To tell Apache to use mod_perl to execute BackupAFS_Admin, add this to Apache&#39;s 1.x httpd.conf file:</p>

<pre><code>    &lt;IfModule mod_perl.c&gt;
        PerlModule Apache::Registry
        PerlTaintCheck On
        &lt;Location /cgi-bin/BackupAFS/BackupAFS_Admin&gt;   # &lt;--- change path as needed
           SetHandler perl-script
           PerlHandler Apache::Registry
           Options ExecCGI
           PerlSendHeader On
        &lt;/Location&gt;
    &lt;/IfModule&gt;</code></pre>

<p>Apache 2.0.44 with Perl 5.8.0 on RedHat 7.1, Don Silvia reports that this works (with tweaks from Michael Tuzi):</p>

<pre><code>    LoadModule perl_module modules/mod_perl.so
    PerlModule Apache2

    &lt;Directory /path/to/cgi/&gt;
        SetHandler perl-script
        PerlResponseHandler ModPerl::Registry
        PerlOptions +ParseHeaders
        Options +ExecCGI
        Order deny,allow
        Deny from all
        Allow from 192.168.0  
        AuthName &quot;Backup Admin&quot;
        AuthType Basic
        AuthUserFile /path/to/user_file
        Require valid-user
    &lt;/Directory&gt;</code></pre>

<p>There are other optimizations and options with mod_perl. For example, you can tell mod_perl to preload various perl modules, which saves memory compared to loading separate copies in every Apache process after they are forked. See Stas&#39;s definitive mod_perl guide at <a href="http://perl.apache.org/guide">http://perl.apache.org/guide</a>.</p>

</dd>
</dl>

<p>BackupAFS_Admin requires that users are authenticated by Apache. Specifically, it expects that Apache sets the REMOTE_USER environment variable when it runs. There are several ways to do this. One way is to create a .htaccess file in the cgi-bin directory that looks like:</p>

<pre><code>    AuthGroupFile /etc/httpd/conf/group    # &lt;--- change path as needed
    AuthUserFile /etc/http/conf/passwd     # &lt;--- change path as needed
    AuthType basic
    AuthName &quot;access&quot;
    require valid-user</code></pre>

<p>You will also need &quot;AllowOverride Indexes AuthConfig&quot; in the Apache httpd.conf file to enable the .htaccess file. Alternatively, everything can go in the Apache httpd.conf file inside a Location directive. The list of users and password file above can be extracted from the NIS passwd file.</p>

<p>One alternative is to use LDAP. In Apache&#39;s http.conf add these lines:</p>

<pre><code>    LoadModule auth_ldap_module   modules/auth_ldap.so
    AddModule auth_ldap.c

    # cgi-bin - auth via LDAP (for BackupAFS)
    &lt;Location /cgi-binBackupAFS/BackupAFS_Admin&gt;    # &lt;--- change path as needed
      AuthType Basic
      AuthName &quot;BackupAFS login&quot;
      # replace MYDOMAIN, PORT, ORG and CO as needed
      AuthLDAPURL ldap://ldap.MYDOMAIN.com:PORT/o=ORG,c=CO?uid?sub?(objectClass=*)
      require valid-user
    &lt;/Location&gt;</code></pre>

<p>If you want to disable the user authentication you can set <a href="#_conf_cgiadminusers_">$Conf{CgiAdminUsers}</a> to &#39;*&#39;, which allows any user to have full access to all VolumeSets and backups. In this case the REMOTE_USER environment variable does not have to be set by Apache.</p>

<p>Alternatively, you can force a particular user name by getting Apache to set REMOTE_USER, eg, to hardcode the user to www you could add this to Apache&#39;s httpd.conf:</p>

<pre><code>    &lt;Location /cgi-bin/BackupAFS/BackupAFS_Admin&gt;   # &lt;--- change path as needed
        Setenv REMOTE_USER www
    &lt;/Location&gt;</code></pre>

<p>Finally, you should also edit the config.pl file and adjust, as necessary, the CGI-specific settings. They&#39;re near the end of the config file. In particular, you should specify which users or groups have administrator (privileged) access: see the config settings <a href="#_conf_cgiadminusergroup_">$Conf{CgiAdminUserGroup}</a> and <a href="#_conf_cgiadminusers_">$Conf{CgiAdminUsers}</a>. Also, the configure.pl script placed various images into <a href="#_conf_cgiimagedir_">$Conf{CgiImageDir}</a> that BackupAFS_Admin needs to serve up. You should make sure that <a href="#_conf_cgiimagedirurl_">$Conf{CgiImageDirURL}</a> is the correct URL for the image directory.</p>

<p>See the section <a>Fixing installation problems</a> for suggestions on debugging the Apache authentication setup.</p>

<h2 id="Step-8:-Running-BackupAFS">Step 8: Running BackupAFS</h2>

<p>The installation contains an init.d backupafs script that can be copied to /etc/init.d so that BackupAFS can auto-start on boot. See init.d/README for further instructions.</p>

<p>BackupAFS should be ready to start. If you installed the init.d script, then you should be able to run BackupAFS with:</p>

<pre><code>    /etc/init.d/backupafs start</code></pre>

<p>(This script can also be invoked with &quot;stop&quot; to stop BackupAFS and &quot;reload&quot; to tell BackupAFS to reload config.pl and the VolumeSet-List file.)</p>

<p>Otherwise, just run</p>

<pre><code>     __INSTALLDIR__/bin/BackupAFS -d</code></pre>

<p>as user backup. The -d option tells BackupAFS to run as a daemon (ie: it does an additional fork).</p>

<p>Any immediate errors will be printed to stderr and BackupAFS will quit. Otherwise, look in /var/log/BackupAFS/LOG and verify that BackupAFS reports it has started and all is ok.</p>

<h2 id="Step-9:-Talking-to-BackupAFS">Step 9: Talking to BackupAFS</h2>

<p>You should verify that BackupAFS is running by using <code>BackupAFS_serverMesg</code>. This sends a message to BackupAFS via the unix (or TCP) socket and prints the response. Like all BackupAFS programs, <code>BackupAFS_serverMesg</code> should be run as the BackupAFS user (backup), so you should</p>

<pre><code>    su backup</code></pre>

<p>before running <code>BackupAFS_serverMesg</code>. If the BackupAFS user is configured with /bin/false as the shell, you can use the -s option to su to explicitly run a shell, eg:</p>

<pre><code>    su -s /bin/bash backup</code></pre>

<p>Depending upon your configuration you might also need the -l option.</p>

<p>You can request status information and start and stop backups using this interface. This socket interface is mainly provided for the CGI interface (and some of the BackupAFS sub-programs use it too). But right now we just want to make sure BackupAFS is happy. Each of these commands should produce some status output:</p>

<pre><code>    __INSTALLDIR__/bin/BackupAFS_serverMesg status info
    __INSTALLDIR__/bin/BackupAFS_serverMesg status jobs
    __INSTALLDIR__/bin/BackupAFS_serverMesg status volsets</code></pre>

<p>The output should be some hashes printed with Data::Dumper. If it looks cryptic and confusing, and doesn&#39;t look like an error message, then all is ok.</p>

<p>The jobs status should initially show just <code>BackupAFS_trashClean</code>. The volsets status should produce a list of every VolumeSet you have listed in /etc/BackupAFS/VolumeSet-List as part of a big cryptic output line.</p>

<p>You can also request that all volsets be queued:</p>

<pre><code>    __INSTALLDIR__/bin/BackupAFS_serverMesg backup all</code></pre>

<p>At this point you should make sure the CGI interface works since it will be much easier to see what is going on. That&#39;s our next subject.</p>

<h2 id="Step-10:-Checking-email-delivery">Step 10: Checking email delivery</h2>

<p>The script <code>BackupAFS_sendEmail</code> sends status and error emails to the administrator and users. It is usually run each night by <code>BackupAFS_nightly</code>.</p>

<p>To verify that it can run sendmail and deliver email correctly you should ask it to send a test email to you:</p>

<pre><code>    su backup
    __INSTALLDIR__/bin/BackupAFS_sendEmail -u MYNAME@MYDOMAIN.COM</code></pre>

<p><code>BackupAFS_sendEmail</code> also takes a -c option that checks if BackupAFS is running, and it sends an email to <a href="#_conf_emailadminusername_">$Conf{EMailAdminUserName}</a> if it is not. That can be used as a keep-alive check by adding</p>

<pre><code>    __INSTALLDIR__/bin/BackupAFS_sendEmail -c</code></pre>

<p>to backup&#39;s cron.</p>

<p>The -t option to <code>BackupAFS_sendEmail</code> causes it to print the email message instead of invoking sendmail to deliver the message.</p>

<h2 id="Other-installation-topics">Other installation topics</h2>

<dl>

<dt id="Removing-a-VolumeSet">Removing a VolumeSet</dt>
<dd>

<p>If there is a volset that no longer needs to be backed up (eg: some volumes are renamed or deleted) you have two choices. First, you can keep the backups accessible and browsable, but disable all new backups. Alternatively, you can completely remove the VolumeSet and all its backups.</p>

<p>To disable backups for a volset <a href="#_conf_backupsdisable_">$Conf{BackupsDisable}</a> can be set to two different values in that VolumeSet&#39;s per-volset config.pl file:</p>

<ol>

<li><p>Don&#39;t do any regular backups on this volset. Manually requested backups (via the CGI interface) will still occur.</p>

</li>
<li><p>Don&#39;t do any backups on this volset. Manually requested backups (via the CGI interface) will be ignored.</p>

</li>
</ol>

</dd>
</dl>

<p>This will still allow the volset&#39;s old backups to be browsable and restorable.</p>

<p>To completely remove a VolumeSet and all its backups, you should remove its entry in the conf/VolumeSet-List file, and then delete the __TOPDIR__/volsets/<i>$volset</i> directory. Whenever you change the VolumeSet-List file, you should send BackupAFS a HUP (-1) signal so that it re-reads the VolumeSet-List file. If you don&#39;t do this, BackupAFS will automatically re-read the VolumeSet-List file at the next regular wakeup.</p>

<h2 id="Fixing-installation-problems">Fixing installation problems</h2>

<p>TODO.</p>

<a href="#_podtop_"><h1 id="Restore-functions">Restore functions</h1></a>

<p>By selecting a volumeset in the CGI interface, a list of all the backups for that volset will be displayed. By selecting the backup number you can navigate the volumes tree for that volumeset.</p>

<p>BackupAFS&#39;s CGI interface automatically displays a <i>merged</i> or <i>filled</i> view when browsing backups. This means that viewing incremental backups shows files from all other backups (full and other incrementals) on which the backup depends, Therefore, there is no need to do multiple restores from the incremental and full backups: BackupAFS does all the hard work for you. You simply select the files and directories you want from the correct backup vintage in one step.</p>

<h2 id="Browser-Download">Browser Download</h2>

<p>You may download a single backup file at any time simply by selecting it. Your browser should prompt you with the file name and ask you whether to open the file or save it to disk (you will wish to save it, &quot;gunzip&quot; and &quot;vos restore&quot; it).</p>

<h2 id="Direct-Restore">Direct Restore</h2>

<p>Alternatively, you can select one or more files or directories in the currently selected directory and select &quot;Restore selected files&quot;. (If you need to restore selected files and directories from several different parent directories you will need to do that in multiple steps.)</p>

<p>If you select all the files in a directory, BackupAFS will replace the list of files with the parent directory. You will be presented with a screen that explains the restore process.</p>

<p>With this method the selected files and directories are restored directly back into AFS, by default with a similar volumename (&quot;.r&quot; is appended unless you change it). You must change the default values for the AFS fileserver and partition to valid values for your AFS cell. You may wish to change the default extension, or leave it as-is. <b>If you remove the extension totally, leaving it blank, any existing volume with the same name will be overwritten, so use caution.</b></p>

<p>Once you select &quot;Start Restore&quot; you will be prompted one last time with a summary of the exact source and target volume(s) before you commit. When you give the final go ahead the restore operation will be queued like a normal backup job, meaning that it will be deferred if there is a backup currently running for that volumeset. When the restore job is run, a &quot;vos restore&quot; operation is used to actually restore the volume(s). There is currently no option to cancel a restore that has been started.</p>

<p>A record of the restore request, including the result and list of volumes, is kept. It can be browsed from the VolumeSet&#39;s home page. <a href="#_conf_restoreinfokeepcnt_">$Conf{RestoreInfoKeepCnt}</a> specifies how many old restore status files to keep.</p>

<p>Note that for direct restore to work, the <a href="#_conf_xfermethod_">$Conf{XferMethod}</a> must be able to write to the destination. Because the only XferMethod available is vos, this means that &quot;vos restore&quot; with the &quot;-localauth&quot; option must succeed. This requires that the AFS cell&#39;s KeyFile be present on the BackupAFS server&#39;s disk. This creates additional security risks, as mentioned in the installation portion of this document.</p>

<a href="#_podtop_"><h1 id="Other-CGI-Functions">Other CGI Functions</h1></a>

<h2 id="Configuration-and-Host-Editor">Configuration and Host Editor</h2>

<p>The CGI interface has a complete configuration and VolumeSet editor. Only the administrator can edit the main configuration settings and VolumeSets. The edit links are in the left navigation bar.</p>

<p>When changes are made to any parameter a &quot;Save&quot; button appears at the top of the page. If you are editing a text box you will need to click outside of the text box to make the Save button appear. If you don&#39;t select Save then the changes won&#39;t be saved.</p>

<p>The volset-specific configuration can be edited from the VolumeSet Summary page using the link in the left navigation bar. The administrator can edit any of the volset-specific configuration settings.</p>

<p>When editing the volset-specific configuration, each parameter has an &quot;override&quot; setting that denotes the value is volset-specific, meaning that it overrides the setting in the main configuration. If you unselect &quot;override&quot; then the setting is removed from the volset-specific configuration, and the main configuration file is displayed.</p>

<p>User&#39;s can edit their volset-specific configuration if enabled via <a href="#_conf_cgiuserconfigeditenable_">$Conf{CgiUserConfigEditEnable}</a>. The specific subset of configuration settings that a user can edit is specified with <a href="#_conf_cgiuserconfigedit_">$Conf{CgiUserConfigEdit}</a>. It is recommended to make this list short as possible (you probably don&#39;t want your users saving hundreds of full backups) and <b>it is essential that they can&#39;t edit any of the Cmd configuration settings, otherwise they can specify an arbitrary command that will be executed as the BackupAFS user!</b></p>

<h2 id="RSS">RSS</h2>

<p>BackupAFS supports a very basic RSS feed. Provided you have the XML::RSS perl module installed, a URL similar to this will provide RSS information:</p>

<pre><code>    http://localhost/cgi-bin/BackupAFS/BackupAFS_Admin?action=rss</code></pre>

<p>This feature is experimental. The information included will probably change.</p>

<a href="#_podtop_"><h1 id="BackupAFS-Design">BackupAFS Design</h1></a>

<h2 id="Some-design-issues">Some design issues</h2>

<dl>

<dt id="Compression">Compression</dt>
<dd>

<p>BackupAFS supports compression. It uses the deflate and inflate methods in the Compress::Zlib module, which is based on the zlib compression library (see <a href="http://www.gzip.org/zlib/">http://www.gzip.org/zlib/</a>) for log files and the gzip (see <a href="http://www.gzip.org/">http://www.gzip.org/</a>) or pigz (see <a href="http://www.zlib.net/pigz/">http://www.zlib.net/pigz/</a>) executable to compress volume dump files. <b>If you have a multi-processor system, pigz is strongly recommended.</b></p>

<p>The <a href="#_conf_compresslevel_">$Conf{CompressLevel}</a> setting specifies the compression level to use. Zero (0) means no compression. Compression levels can be from 1 (least cpu time, slightly worse compression) to 9 (most cpu time, slightly better compression). The recommended value is 3 or 4. Changing it to 5, for example, will take maybe 20% more cpu time and will get another 2-3% additional compression. Diminishing returns set in above 5. See the gzip documentation for more information about compression levels.</p>

<p>Using compression can yield a 35% or more overall saving in backup storage.</p>

</dd>
</dl>

<h2 id="BackupAFS-operation">BackupAFS operation</h2>

<p>BackupAFS reads the configuration information from /etc/BackupAFS/config.pl. It then runs and manages all the backup activity. It maintains queues of pending backup requests, user backup requests and administrative commands. Based on the configuration various requests will be executed simultaneously.</p>

<p>As specified by <a href="#_conf_wakeupschedule_">$Conf{WakeupSchedule}</a>, BackupAFS wakes up periodically to queue backups on all the VolumeSets. This is a four step process:</p>

<ol>

<li><p>For each VolumeSet backup requests are queued on the background command queue.</p>

</li>
<li><p>For each VolumeSet, BackupAFS_dump is forked. Several of these may be run in parallel, based on the configuration. The file __TOPDIR__/volsets/<i>VolumeSet Name</i>/backups is read to decide whether a full or incremental backup needs to be run. If no backup is scheduled, then BackupAFS_dump exits.</p>

<p>The backup is done using the specified XferMethod. The only XferMethod available in BackupAFS is vos. BackupAFS_dump uses BackupAFS_getVols to construct a list of volumes for the given VolumeSet. It does this by first reading the list of volume entries for the given VolumeSet. Then, by querying the AFS dbservers for a list of all fileservers, querying each fileserver for a list of partitions, and querying the VLDB for a list of volumes on each matching server/partition, each volume matching the criteria is returned. Then BackupAFS_vosWrapper is spawned for each volume. Depending on the type of dump, it determines whether a backup is necessary, and if so, performs the actual &quot;vos dump&quot; with the correct arguments.</p>

<p>The volume dump files are stored in __TOPDIR__/volsets/<i>VolumeSet Name</i>/new. The XferMethod output is stored into __TOPDIR__/volsets/&lt;VolumeSet Name&gt;/XferLOG.</p>

<p>When the entire list of matching volumes has been processed, the forked BackupAFS_dump exits.</p>

</li>
<li><p>For each complete, good, backup, BackupAFS_compress is run. To avoid excessive resource contention, only a single BackupAFS_compress program runs at a time and the rest are queued.</p>

<p>BackupAFS_compress reads the NewFileList written by BackupAFS_dump and if compression has been requested, it spawns a gzip or pigz process to compress the sequentially for each file. If both gzip and pigz executables are available, it will use pigz. Pigz defaults to a number of threads equal to the number of cores available on the BackupAFS system. This can be overridden by specifying or changing the value of <a href="#_conf_pigzthreads_">$Conf{PigzThreads}</a>. Overriding this value allows the admin to fine-tune the CPU and IO usage of pigz.</p>

<p>The CGI interface knows how to merge each incremental backups with all lower-level dumps on which it depends, giving the incremental backups a complete appearance.</p>

</li>
<li><p>BackupAFS_trashClean is always run in the background to remove any expired backups. Every 5 minutes it wakes up and removes all the files in __TOPDIR__/trash.</p>

<p>Also, once each night, BackupAFS_nightly is run to complete some additional administrative tasks, such as aging of log files. To avoid race conditions, BackupAFS_nightly is only run when there are no BackupAFS_compress processes running. When BackupAFS_nightly is run no new BackupAFS_compress jobs are started.</p>

</li>
</ol>

<p>BackupAFS also listens for TCP connections on <a href="#_conf_serverport_">$Conf{ServerPort}</a>, which is used by the CGI script BackupAFS_Admin for status reporting and user-initiated backup or backup cancel requests.</p>

<h2 id="Storage-layout">Storage layout</h2>

<p>BackupAFS resides in several directories:</p>

<dl>

<dt id="INSTALLDIR__">__INSTALLDIR__</dt>
<dd>

<p>Perl scripts comprising BackupAFS reside in __INSTALLDIR__/bin, libraries are in __INSTALLDIR__/lib and documentation is in __INSTALLDIR__/doc.</p>

</dd>
<dt id="usr-lib-cgi-bin">/usr/lib/cgi-bin</dt>
<dd>

<p>The CGI script BackupAFS_Admin resides in this cgi binary directory.</p>

</dd>
<dt id="etc-BackupAFS">/etc/BackupAFS</dt>
<dd>

<p>All the configuration information resides below /etc/BackupAFS. This directory contains:</p>

<p>The directory /etc/BackupAFS contains:</p>

<dl>

<dt id="config.pl">config.pl</dt>
<dd>

<p>Configuration file. See <a>Configuration file</a> below for more details.</p>

</dd>
<dt id="VolumeSet-List">VolumeSet-List</dt>
<dd>

<p>Text file, which lists and defines all the VolumeSets to backup.</p>

</dd>
<dt id="volsets">volsets</dt>
<dd>

<p>The directory /etc/BackupAFS/volsets contains per-volumeset configuration files that override settings in the main configuration file. Each file is named /etc/BackupAFS/volset/VOLSET.pl, where VOLSET is the name of the VolumeSet.</p>

</dd>
</dl>

</dd>
<dt id="var-log-BackupAFS">/var/log/BackupAFS</dt>
<dd>

<p>The directory /var/log/BackupAFS (__TOPDIR__/log on pre-FHS versions of BackupAFS) contains:</p>

<dl>

<dt id="LOG">LOG</dt>
<dd>

<p>Current (today&#39;s) log file output from BackupAFS.</p>

</dd>
<dt id="LOG.0-or-LOG.0.z">LOG.0 or LOG.0.z</dt>
<dd>

<p>Yesterday&#39;s log file output. Log files are aged daily and compressed (if compression is enabled), and old LOG files are deleted.</p>

</dd>
<dt id="BackupAFS.pid">BackupAFS.pid</dt>
<dd>

<p>Contains BackupAFS&#39;s process id.</p>

</dd>
<dt id="status.pl">status.pl</dt>
<dd>

<p>A summary of BackupAFS&#39;s status written periodically by BackupAFS so that certain state information can be maintained if BackupAFS is restarted. Should not be edited.</p>

</dd>
<dt id="UserEmailInfo.pl">UserEmailInfo.pl</dt>
<dd>

<p>A summary of what email was last sent to each user, and when the last email was sent. Should not be edited.</p>

</dd>
</dl>

</dd>
<dt id="TOPDIR__">__TOPDIR__</dt>
<dd>

<p>All of BackupAFS&#39;s data (each VolumeSet&#39;s &quot;vos dump&quot; files, logs, backups files) is stored below this directory.</p>

<p>Below __TOPDIR__ are several directories:</p>

<dl>

<dt id="TOPDIR__-trash">__TOPDIR__/trash</dt>
<dd>

<p>Any directories and files below this directory are periodically deleted whenever BackupAFS_trashClean checks. When a backup is aborted or when an old backup expires, BackupAFS_dump simply moves the directory to __TOPDIR__/trash for later removal by BackupAFS_trashClean.</p>

</dd>
<dt id="TOPDIR__-volsets-VolumeSet_name">__TOPDIR__/volsets/<i>VolumeSet_name</i></dt>
<dd>

<p>For each VolumeSet, all the backups for that volset are stored below the directory __TOPDIR__/volsets/<i>VolumeSet_name</i>. This directory contains the following files:</p>

<dl>

<dt id="LOG1">LOG</dt>
<dd>

<p>Current log file for this VolumeSet from BackupAFS_dump.</p>

</dd>
<dt id="LOG.DDMMYYYY-or-LOG.DDMMYYYY.z">LOG.DDMMYYYY or LOG.DDMMYYYY.z</dt>
<dd>

<p>Last month&#39;s log file. Log files are aged monthly and compressed (if compression is enabled), and old LOG files are deleted. In earlier versions of BackupAFS these files used to have a suffix of 0, 1, ....</p>

</dd>
<dt id="XferERR-or-XferERR.z">XferERR or XferERR.z</dt>
<dd>

<p>Output from the transport program (ie vos) for the most recent failed backup.</p>

</dd>
<dt id="new">new</dt>
<dd>

<p>Subdirectory in which the current backup is stored. This directory is renamed if the backup succeeds.</p>

</dd>
<dt id="XferLOG-or-XferLOG.z">XferLOG or XferLOG.z</dt>
<dd>

<p>Output from the transport program (ie vos) for the current backup.</p>

</dd>
<dt id="nnn-an-integer">nnn (an integer)</dt>
<dd>

<p>Successful backups are in directories numbered sequentially starting at 0. These numbers <b>do not necessarily correspond to incr dump levels</b>.</p>

</dd>
<dt id="XferLOG.nnn-or-XferLOG.nnn.z">XferLOG.nnn or XferLOG.nnn.z</dt>
<dd>

<p>Output from the transport program (ie vos) corresponding to backup number nnn. <b>Note that the restore numbers are not related to the backup number.</b></p>

</dd>
<dt id="RestoreInfo.nnn">RestoreInfo.nnn</dt>
<dd>

<p>Information about restore request #nnn including who, what, when, and why. This file is in Data::Dumper format. <b>Note that the restore numbers are not related to the backup number.</b></p>

</dd>
<dt id="RestoreLOG.nnn.z">RestoreLOG.nnn.z</dt>
<dd>

<p>Output from smbclient, tar or rsync during restore #nnn. (Note that the restore numbers are not related to the backup number.)</p>

</dd>
<dt id="backups">backups</dt>
<dd>

<p>A tab-delimited ascii table listing information about each successful backup, one per row. The columns are:</p>

<dl>

<dt id="num">num</dt>
<dd>

<p>The backup number, an integer that starts at 0 and increments for each successive backup. The corresponding backup is stored in the directory num (eg: if this field is 5, then the backup is stored in __TOPDIR__/volsets/$VolumeSet/5).</p>

</dd>
<dt id="type">type</dt>
<dd>

<p>Set to &quot;full&quot; or &quot;incr&quot; for full or incremental backup.</p>

</dd>
<dt id="startTime">startTime</dt>
<dd>

<p>Start time of the backup in unix seconds.</p>

</dd>
<dt id="endTime">endTime</dt>
<dd>

<p>Stop time of the backup in unix seconds.</p>

</dd>
<dt id="nFiles">nFiles</dt>
<dd>

<p>Number of files backed up (as reported by the transport mechanism).</p>

</dd>
<dt id="size">size</dt>
<dd>

<p>Total file size backed up (as reported by the transport mechanism).</p>

</dd>
<dt id="nFilesExist">nFilesExist</dt>
<dd>

<p>Number of files that were already in the pool (BackupAFS does not use this field).</p>

</dd>
<dt id="sizeExist">sizeExist</dt>
<dd>

<p>Total size of files that were already in the pool (BackupAFS does not use this field).</p>

</dd>
<dt id="nFilesNew">nFilesNew</dt>
<dd>

<p>Number of files that were not in the pool (as determined by BackupAFS_vosWrapper).</p>

</dd>
<dt id="sizeNew">sizeNew</dt>
<dd>

<p>Total size of files that were backed up (as determined by BackupAFS_vosWrapper).</p>

</dd>
<dt id="xferErrs">xferErrs</dt>
<dd>

<p>Number of errors or warnings from vos.</p>

</dd>
<dt id="xferBadFile">xferBadFile</dt>
<dd>

<p>Number of errors from smbclient that were bad file errors (zero otherwise). (BackupAFS does not use this field).</p>

</dd>
<dt id="xferBadShare">xferBadShare</dt>
<dd>

<p>Number of errors from smbclient that were bad share errors (zero otherwise). (BackupAFS does not use this field).</p>

</dd>
<dt id="tarErrs">tarErrs</dt>
<dd>

<p>Number of errors from BackupAFS_tarExtract. (BackupAFS does not use this field).</p>

</dd>
<dt id="compress">compress</dt>
<dd>

<p>The compression level used on this backup. Zero or empty means no compression.</p>

</dd>
<dt id="sizeExistComp">sizeExistComp</dt>
<dd>

<p>Total compressed size of files that were already in the pool (BackupAFS does not use this field).</p>

</dd>
<dt id="sizeNewComp">sizeNewComp</dt>
<dd>

<p>Total compressed size of files that were not in the pool (as determined by BackupAFS_compress).</p>

</dd>
<dt id="noFill">noFill</dt>
<dd>

<p>Set if this backup has not been filled in with the most recent previous filled or full backup. This will/should always be set to 1 in BackupAFS.</p>

</dd>
<dt id="fillFromNum">fillFromNum</dt>
<dd>

<p>If this backup was filled (ie: noFill is 0) then this is the number of the backup that it was filled from. (BackupAFS does not use this field).</p>

</dd>
<dt id="mangle">mangle</dt>
<dd>

<p>Set if this backup has mangled file names and attributes. Always false for backups in BackupAFS. True for backups created with BackupPC4AFS. BackupAFS comes with a &quot;BackupAFS_migrate_unmangle_datadir&quot; script which can be used to un-mangle the directory and filenames created by BackupPC4AFS.</p>

</dd>
<dt id="xferMethod">xferMethod</dt>
<dd>

<p>Set to the value of <a href="#_conf_xfermethod_">$Conf{XferMethod}</a> when this dump was done. For BackupAFS, &quot;vos&quot; is currently the only available XferMethod.</p>

</dd>
<dt id="level">level</dt>
<dd>

<p>The level of this dump. A full dump is level 0. Incrementals are an integer 1-9.</p>

</dd>
</dl>

</dd>
<dt id="restores">restores</dt>
<dd>

<p>A tab-delimited ascii table listing information about each requested restore, one per row. The columns are:</p>

<dl>

<dt id="num1">num</dt>
<dd>

<p>Restore number (matches the suffix of the RestoreInfo.nnn and RestoreLOG.nnn.z file), unrelated to the backup number.</p>

</dd>
<dt id="startTime1">startTime</dt>
<dd>

<p>Start time of the restore in unix seconds.</p>

</dd>
<dt id="endTime1">endTime</dt>
<dd>

<p>End time of the restore in unix seconds.</p>

</dd>
<dt id="result">result</dt>
<dd>

<p>Result (ok or failed).</p>

</dd>
<dt id="errorMsg">errorMsg</dt>
<dd>

<p>Error message if restore failed.</p>

</dd>
<dt id="nFiles1">nFiles</dt>
<dd>

<p>Number of files restored.</p>

</dd>
<dt id="size1">size</dt>
<dd>

<p>Size in bytes of the restored files.</p>

</dd>
<dt id="tarCreateErrs">tarCreateErrs</dt>
<dd>

<p>Number of errors from BackupAFS_tarCreate during restore. (BackupAFS does not use this field).</p>

</dd>
<dt id="xferErrs1">xferErrs</dt>
<dd>

<p>Number of errors from vos during restore.</p>

</dd>
</dl>

</dd>
</dl>

</dd>
</dl>

</dd>
</dl>

<h2 id="File-name-mangling">File name mangling</h2>

<p>Backup file names are no longer stored in &quot;mangled&quot; form. In BackupPC4AFS (and in BackupPC), each node of a path is preceded by &quot;f&quot; (mnemonic: file), and special characters (\n, \r, % and /) are URI-encoded as &quot;%xx&quot;, where xx is the ascii character&#39;s hex value. So c:/craig/example.txt is now stored as fc/fcraig/fexample.txt.</p>

<p>This was done mainly so meta-data could be stored alongside the backup files without name collisions. In particular, the attributes for the files in a directory are stored in a file called &quot;attrib&quot;, and mangling avoids file name collisions (I discarded the idea of having a duplicate directory tree for every backup just to store the attributes). Other meta-data (eg: rsync checksums) could be stored in file names preceded by, eg, &quot;c&quot;. There are two other benefits to mangling: the share name might contain &quot;/&quot; (eg: &quot;/home/craig&quot; for tar transport), and I wanted that represented as a single level in the storage tree. Secondly, as files are written to NewFileList for later processing by BackupAFS_compress, embedded newlines in the file&#39;s path will cause problems which are avoided by mangling.</p>

<p>The CGI script undoes the mangling, so it is invisible to the user. Both mangled and unmangled backups are still viewable by the CGI interface as long as the entry in the backups file is correctly set for that dump.</p>

<h2 id="Limitations">Limitations</h2>

<p>BackupAFS isn&#39;t perfect (but it is getting better). Please see <a href="http://backupafs.sourceforge.net/faq/limitations.html">http://backupafs.sourceforge.net/faq/limitations.html</a> for a discussion of some of BackupAFS&#39;s limitations.</p>

<h2 id="Security-issues">Security issues</h2>

<p>In order to perform vos dump operations without tokens, we need to copy the AFS cell&#39;s keyfile to the BackupAFS server. This keyfile must be protected. Only the backupafs user should be able to read it. This means that the BackupAFS server should be kept as secure as your AFS fileservers. This document won&#39;t devolve into a security lecture, but suffice it to say you should turn off all unnecessary services and wrap or firewall the remaining services.</p>

<a href="#_podtop_"><h1 id="Configuration-File">Configuration File</h1></a>

<p>The BackupAFS configuration file resides in /etc/BackupAFS/config.pl. Optional per-PC configuration files reside in /etc/BackupAFS/volsets/$VolumeSet.pl This file can be used to override settings just for a particular PC.</p>

<h2 id="Modifying-the-main-configuration-file">Modifying the main configuration file</h2>

<p>The configuration file is a perl script that is executed by BackupAFS, so you should be careful to preserve the file syntax (punctuation, quotes etc) when you edit it. It is recommended that you use CVS, RCS or some other method of source control for changing config.pl.</p>

<p>BackupAFS reads or re-reads the main configuration file and the VolumeSet-List file in three cases:</p>

<ul>

<li><p>Upon startup.</p>

</li>
<li><p>When BackupAFS is sent a HUP (-1) signal. Assuming you installed the init.d script, you can also do this with &quot;/etc/init.d/backupafs reload&quot;.</p>

</li>
<li><p>When the modification time of config.pl file changes. BackupAFS checks the modification time once during each regular wakeup.</p>

</li>
</ul>

<p>Whenever you change the configuration file you can either do a kill -HUP BackupAFS_pid or simply wait until the next regular wakeup period.</p>

<p>Each time the configuration file is re-read a message is reported in the LOG file, so you can tail it (or view it via the CGI interface) to make sure your kill -HUP worked. Errors in parsing the configuration file are also reported in the LOG file.</p>

<p>The optional per-VolumeSet configuration file (/etc/BackupAFS/volsets/<i>VolumeSet_Name</i>.pl is read whenever it is needed by BackupAFS_dump, BackupAFS_compress and others.</p>

<a href="#_podtop_"><h1 id="Configuration-Parameters">Configuration Parameters</h1></a>

<p>The configuration parameters are divided into five general groups. The first group (general server configuration) provides general configuration for BackupAFS. The next two groups describe what to backup, when to do it, and how long to keep it. The fourth group are settings for email reminders, and the final group contains settings for the CGI interface.</p>

<p>All configuration settings in the second through fifth groups can be overridden by the per-VolumeSet config.pl file.</p>

<h2 id="General-server-configuration">General server configuration</h2>

<dl>

<dt id="Conf-ServerHost"><a href="#_conf_serverhost_">$Conf{ServerHost}</a> = &#39;&#39;;</dt>
<dd>

<p>Host name on which the BackupAFS server is running.</p>

</dd>
<dt id="Conf-ServerPort--1"><a href="#_conf_serverport_">$Conf{ServerPort}</a> = -1;</dt>
<dd>

<p>TCP port number on which the BackupAFS server listens for and accepts connections. Normally this should be disabled (set to -1). The TCP port is only needed if apache runs on a different machine from BackupAFS. In that case, set this to any spare port number over 1024 (eg: 2359). If you enable the TCP port, make sure you set <a href="#_conf_servermesgsecret_">$Conf{ServerMesgSecret}</a> too!</p>

</dd>
<dt id="Conf-ServerMesgSecret"><a href="#_conf_servermesgsecret_">$Conf{ServerMesgSecret}</a> = &#39;&#39;;</dt>
<dd>

<p>Shared secret to make the TCP port secure. Set this to a hard to guess string if you enable the TCP port (ie: <a href="#_conf_serverport_">$Conf{ServerPort}</a> &gt; 0).</p>

<p>To avoid possible attacks via the TCP socket interface, every client message is protected by an MD5 digest. The MD5 digest includes four items: - a seed that is sent to the client when the connection opens - a sequence number that increments for each message - a shared secret that is stored in <a href="#_conf_servermesgsecret_">$Conf{ServerMesgSecret}</a> - the message itself.</p>

<p>The message is sent in plain text preceded by the MD5 digest. A snooper can see the plain-text seed sent by BackupAFS and plain-text message from the client, but cannot construct a valid MD5 digest since the secret <a href="#_conf_servermesgsecret_">$Conf{ServerMesgSecret}</a> is unknown. A replay attack is not possible since the seed changes on a per-connection and per-message basis.</p>

</dd>
<dt id="Conf-MyPath-bin"><a href="#_conf_mypath_">$Conf{MyPath}</a> = &#39;/bin&#39;;</dt>
<dd>

<p>PATH setting for BackupAFS. An explicit value is necessary for taint mode. Value shouldn&#39;t matter too much since all execs use explicit paths. However, taint mode in perl will complain if this directory is world writable.</p>

</dd>
<dt id="Conf-UmaskMode-027"><a href="#_conf_umaskmode_">$Conf{UmaskMode}</a> = 027;</dt>
<dd>

<p>Permission mask for directories and files created by BackupAFS. Default value prevents any access from group other, and prevents group write.</p>

</dd>
<dt id="Conf-WakeupSchedule-23"><a href="#_conf_wakeupschedule_">$Conf{WakeupSchedule}</a> = [23];</dt>
<dd>

<p>Times at which we wake up, check all the volumesets, and schedule necessary backups. Times are measured in hours since midnight. Can be fractional if necessary (eg: 4.25 means 4:15am).</p>

<p>If the volsets you are backing up are always available (ie, not at the end of a dialup link, or are not in another country), you might only have one wakeup each night.</p>

<p>On the other hand, if you are backing up volumesets that are only intermittently connected to the network you might need multiple wakeup times.</p>

<p>Examples:</p>

<pre><code>    <a href="#_conf_wakeupschedule_">$Conf{WakeupSchedule}</a> = [22.5];         # once per day at 10:30 pm.
    <a href="#_conf_wakeupschedule_">$Conf{WakeupSchedule}</a> = [2,4,6,8,10,12,14,16,18,20,22];  # every 2 hours</code></pre>

<p>The default value is 23 (11:00PM).</p>

<p>The first entry of <a href="#_conf_wakeupschedule_">$Conf{WakeupSchedule}</a> is when BackupAFS_nightly is run. You might want to re-arrange the entries in <a href="#_conf_wakeupschedule_">$Conf{WakeupSchedule}</a> (they don&#39;t have to be ascending) so that the first entry is when you want BackupAFS_nightly to run (eg: when you don&#39;t expect a lot of regular backups to run).</p>

</dd>
<dt id="Conf-MaxBackups-4"><a href="#_conf_maxbackups_">$Conf{MaxBackups}</a> = 4;</dt>
<dd>

<p>Maximum number of simultaneous backups to run. If there are no user backup requests then this is the maximum number of simultaneous backups.</p>

</dd>
<dt id="Conf-MaxUserBackups-4"><a href="#_conf_maxuserbackups_">$Conf{MaxUserBackups}</a> = 4;</dt>
<dd>

<p>Additional number of simultaneous backups that users can run. As many as <a href="#_conf_maxbackups_">$Conf{MaxBackups}</a> + <a href="#_conf_maxuserbackups_">$Conf{MaxUserBackups}</a> requests can run at the same time.</p>

</dd>
<dt id="Conf-MaxPendingCmds-15"><a href="#_conf_maxpendingcmds_">$Conf{MaxPendingCmds}</a> = 15;</dt>
<dd>

<p>Maximum number of pending link commands. New backups will only be started if there are no more than <a href="#_conf_maxpendingcmds_">$Conf{MaxPendingCmds}</a> plus <a href="#_conf_maxbackups_">$Conf{MaxBackups}</a> number of pending link commands, plus running jobs. This limit is to make sure BackupAFS doesn&#39;t fall too far behind in running BackupAFS_compress commands.</p>

</dd>
<dt id="Conf-CmdQueueNice-10"><a href="#_conf_cmdqueuenice_">$Conf{CmdQueueNice}</a> = 10;</dt>
<dd>

<p>Nice level at which CmdQueue commands (eg: BackupAFS_compress and BackupAFS_nightly) are run at.</p>

</dd>
<dt id="Conf-MaxBackupAFSNightlyJobs-2"><a href="#_conf_maxbackupafsnightlyjobs_">$Conf{MaxBackupAFSNightlyJobs}</a> = 2;</dt>
<dd>

<p>How many BackupAFS_nightly processes to run in parallel.</p>

<p>Each night, at the first wakeup listed in <a href="#_conf_wakeupschedule_">$Conf{WakeupSchedule}</a>, BackupAFS_nightly is run. Its job is to remove unneeded files in the pool, ie: files that only have one link. To avoid race conditions, BackupAFS_nightly and BackupAFS_compress cannot run at the same time. Starting in v1.0.0, BackupAFS_nightly can run concurrently with backups (BackupAFS_dump).</p>

<p>So to reduce the elapsed time, you might want to increase this setting to run several BackupAFS_nightly processes in parallel (eg: 4, or even 8).</p>

</dd>
<dt id="Conf-BackupAFSNightlyPeriod-1"><a href="#_conf_backupafsnightlyperiod_">$Conf{BackupAFSNightlyPeriod}</a> = 1;</dt>
<dd>

<p>This setting is a reference to additional work which BackupAFS_nightly used to do in BackupPC. It is not used in BackupAFS.</p>

</dd>
<dt id="Conf-MaxOldLogFiles-14"><a href="#_conf_maxoldlogfiles_">$Conf{MaxOldLogFiles}</a> = 14;</dt>
<dd>

<p>Maximum number of log files we keep around in log directory. These files are aged nightly. A setting of 14 means the log directory will contain about 2 weeks of old log files, in particular at most the files LOG, LOG.0, LOG.1, ... LOG.13 (except today&#39;s LOG, these files will have a .z extension if compression is on).</p>

<p>If you decrease this number after BackupAFS has been running for a while you will have to manually remove the older log files.</p>

</dd>
<dt id="Conf-DfPath"><a href="#_conf_dfpath_">$Conf{DfPath}</a> = &#39;&#39;;</dt>
<dd>

<p>Full path to the df command. Security caution: normal users should not allowed to write to this file or directory.</p>

</dd>
<dt id="Conf-DfCmd-dfPath-topDir"><a href="#_conf_dfcmd_">$Conf{DfCmd}</a> = &#39;$dfPath $topDir&#39;;</dt>
<dd>

<p>Command to run df. The following variables are substituted at run-time:</p>

<pre><code>  $dfPath      path to df (<a href="#_conf_dfpath_">$Conf{DfPath}</a>)
  $topDir      top-level BackupAFS data directory</code></pre>

<p>Note: all Cmds are executed directly without a shell, so the prog name needs to be a full path and you can&#39;t include shell syntax like redirection and pipes; put that in a script if you need it.</p>

</dd>
<dt id="Conf-AfsVosBackupArgs---volume-shareName---type-type---incrDate-incrDate---incrLevel-incrLevel---clientDir-topDir-volsets-client---dest-topDir-volsets-client-new"><a href="#_conf_afsvosbackupargs_">$Conf{AfsVosBackupArgs}</a> = &#39;--volume=$shareName --type=$type --incrDate=$incrDate --incrLevel=$incrLevel --clientDir=$topDir/volsets/$client --dest=$topDir/volsets/$client/new&#39;;</dt>
<dd>

</dd>
<dt id="Conf-AfsVosRestoreArgs---volume-shareName---type-type---clientDir-topDir-volsets-client---restoreDir-restoreDir---bkupSrcNum-bkupSrcNum---bkupSrcVolSet-bkupSrcVolSet---fileList-fileList"><a href="#_conf_afsvosrestoreargs_">$Conf{AfsVosRestoreArgs}</a> = &#39;--volume=$shareName --type=$type --clientDir=$topDir/volsets/$client --restoreDir=$restoreDir --bkupSrcNum=$bkupSrcNum --bkupSrcVolSet=$bkupSrcVolSet --fileList=$fileList&#39;;</dt>
<dd>

<p>Arguments that are passed from BackupAFS_dump to BackupAFS_vosWrapper. It is probably not a good idea to edit these unless you are a BackupAFS developer.</p>

</dd>
<dt id="Conf-AfsVosPath"><a href="#_conf_afsvospath_">$Conf{AfsVosPath}</a> = &#39;&#39;;</dt>
<dd>

</dd>
<dt id="Conf-CatPath"><a href="#_conf_catpath_">$Conf{CatPath}</a> = &#39;&#39;;</dt>
<dd>

</dd>
<dt id="Conf-GzipPath"><a href="#_conf_gzippath_">$Conf{GzipPath}</a> = &#39;&#39;;</dt>
<dd>

</dd>
<dt id="Conf-PigzPath"><a href="#_conf_pigzpath_">$Conf{PigzPath}</a> = &#39;&#39;;</dt>
<dd>

<p>Full path to various commands used by BackupAFS.</p>

</dd>
<dt id="Conf-DfMaxUsagePct-95"><a href="#_conf_dfmaxusagepct_">$Conf{DfMaxUsagePct}</a> = 95;</dt>
<dd>

<p>Maximum threshold for disk utilization on the __TOPDIR__ filesystem. If the output from <a href="#_conf_dfpath_">$Conf{DfPath}</a> reports a percentage larger than this number then no new regularly scheduled backups will be run. However, user requested backups (which are usually incremental and tend to be small) are still performed, independent of disk usage. Also, currently running backups will not be terminated when the disk usage exceeds this number.</p>

</dd>
<dt id="Conf-TrashCleanSleepSec-300"><a href="#_conf_trashcleansleepsec_">$Conf{TrashCleanSleepSec}</a> = 300;</dt>
<dd>

<p>How long BackupAFS_trashClean sleeps in seconds between each check of the trash directory. Once every 5 minutes should be reasonable.</p>

</dd>
<dt id="Conf-BackupAFSUser"><a href="#_conf_backupafsuser_">$Conf{BackupAFSUser}</a> = &#39;&#39;;</dt>
<dd>

<p>The BackupAFS user.</p>

</dd>
<dt id="Conf-TopDir"><a href="#_conf_topdir_">$Conf{TopDir}</a> = &#39;&#39;;</dt>
<dd>

</dd>
<dt id="Conf-ConfDir"><a href="#_conf_confdir_">$Conf{ConfDir}</a> = &#39;&#39;;</dt>
<dd>

</dd>
<dt id="Conf-LogDir"><a href="#_conf_logdir_">$Conf{LogDir}</a> = &#39;&#39;;</dt>
<dd>

</dd>
<dt id="Conf-InstallDir"><a href="#_conf_installdir_">$Conf{InstallDir}</a> = &#39;&#39;;</dt>
<dd>

</dd>
<dt id="Conf-CgiDir"><a href="#_conf_cgidir_">$Conf{CgiDir}</a> = &#39;&#39;;</dt>
<dd>

<p>Important installation directories:</p>

<pre><code>  TopDir     - where all the backup data is stored
  ConfDir    - where the main config and VolumeSet-List files resides
  LogDir     - where log files and other transient information
  InstallDir - where the bin, lib and doc installation dirs reside.
               Note: you cannot change this value since all the
               perl scripts include this path. You must reinstall
               with configure.pl to change InstallDir.
  CgiDir     - Apache CGI directory for BackupAFS_Admin</code></pre>

<p>Note: it is STRONGLY recommended that you don&#39;t change the values here. These are set at installation time and are here for reference and are used during upgrades.</p>

<p>Instead of changing TopDir here it is recommended that you use a symbolic link to the new location, or mount the new BackupAFS store at the existing <a href="#_conf_topdir_">$Conf{TopDir}</a> setting.</p>

</dd>
<dt id="Conf-BackupAFSUserVerify-1"><a href="#_conf_backupafsuserverify_">$Conf{BackupAFSUserVerify}</a> = 1;</dt>
<dd>

<p>Whether BackupAFS and the CGI script BackupAFS_Admin verify that they are really running as user <a href="#_conf_backupafsuser_">$Conf{BackupAFSUser}</a>. If this flag is set and the effective user id (euid) differs from <a href="#_conf_backupafsuser_">$Conf{BackupAFSUser}</a> then both scripts exit with an error. This catches cases where BackupAFS might be accidently started as root or the wrong user, or if the CGI script is not installed correctly.</p>

</dd>
<dt id="Conf-PerlModuleLoad-undef"><a href="#_conf_perlmoduleload_">$Conf{PerlModuleLoad}</a> = undef;</dt>
<dd>

<p>Advanced option for asking BackupAFS to load additional perl modules. Can be a list (array ref) of module names to load at startup.</p>

</dd>
<dt id="Conf-ServerInitdPath"><a href="#_conf_serverinitdpath_">$Conf{ServerInitdPath}</a> = &#39;&#39;;</dt>
<dd>

</dd>
<dt id="Conf-ServerInitdStartCmd"><a href="#_conf_serverinitdstartcmd_">$Conf{ServerInitdStartCmd}</a> = &#39;&#39;;</dt>
<dd>

<p>Path to init.d script and command to use that script to start the server from the CGI interface. The following variables are substituted at run-time:</p>

<pre><code>  $sshPath           path to ssh (<a href="#_conf_sshpath_">$Conf{SshPath}</a>)
  $serverHost        same as <a href="#_conf_serverhost_">$Conf{ServerHost}</a>
  $serverInitdPath   path to init.d script (<a href="#_conf_serverinitdpath_">$Conf{ServerInitdPath}</a>)</code></pre>

<p>Example:</p>

<p><a href="#_conf_serverinitdpath_">$Conf{ServerInitdPath}</a> = &#39;/etc/init.d/backupafs&#39;; <a href="#_conf_serverinitdstartcmd_">$Conf{ServerInitdStartCmd}</a> = &#39;$sshPath -q -x -l root $serverHost&#39; . &#39; $serverInitdPath start&#39; . &#39; &lt; /dev/null &gt;&amp; /dev/null&#39;;</p>

<p>Note: all Cmds are executed directly without a shell, so the prog name needs to be a full path and you can&#39;t include shell syntax like redirection and pipes; put that in a script if you need it.</p>

</dd>
</dl>

<h2 id="What-to-backup-and-when-to-do-it">What to backup and when to do it</h2>

<dl>

<dt id="Conf-FullPeriod-6.97"><a href="#_conf_fullperiod_">$Conf{FullPeriod}</a> = 6.97;</dt>
<dd>

<p>Minimum period in days between full backups. A full dump will only be done if at least this much time has elapsed since the last full dump, and at least <a href="#_conf_incrperiod_">$Conf{IncrPeriod}</a> days has elapsed since the last successful dump.</p>

<p>Typically this is set slightly less than an integer number of days. The time taken for the backup, plus the granularity of <a href="#_conf_wakeupschedule_">$Conf{WakeupSchedule}</a> will make the actual backup interval a bit longer.</p>

</dd>
<dt id="Conf-IncrPeriod-0.97"><a href="#_conf_incrperiod_">$Conf{IncrPeriod}</a> = 0.97;</dt>
<dd>

<p>Minimum period in days between incremental backups (a user- or admin-requested incremental backup will be done anytime on demand).</p>

<p>Typically this is set slightly less than an integer number of days. The time taken for the backup, plus the granularity of <a href="#_conf_wakeupschedule_">$Conf{WakeupSchedule}</a> will make the actual backup interval a bit longer.</p>

</dd>
<dt id="Conf-FullKeepCnt-1"><a href="#_conf_fullkeepcnt_">$Conf{FullKeepCnt}</a> = 1;</dt>
<dd>

<p>Number of full backups to keep. Must be &gt;= 1.</p>

<p>In the steady state, each time a full backup completes successfully the oldest one is removed. If this number is decreased, the extra old backups will be removed.</p>

<p>If filling of incremental dumps is off the oldest backup always has to be a full (ie: filled) dump. This might mean one or two extra full dumps are kept until the oldest incremental backups expire.</p>

<p>Exponential backup expiry is also supported. This allows you to specify:</p>

<pre><code>  - num fulls to keep at intervals of 1 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>, followed by
  - num fulls to keep at intervals of 2 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>,
  - num fulls to keep at intervals of 4 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>,
  - num fulls to keep at intervals of 8 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>,
  - num fulls to keep at intervals of 16 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>,</code></pre>

<p>and so on. This works by deleting every other full as each expiry boundary is crossed.</p>

<p>Exponential expiry is specified using an array for <a href="#_conf_fullkeepcnt_">$Conf{FullKeepCnt}</a>:</p>

<pre><code>  <a href="#_conf_fullkeepcnt_">$Conf{FullKeepCnt}</a> = [4, 2, 3];</code></pre>

<p>Entry #n specifies how many fulls to keep at an interval of 2^n * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a> (ie: 1, 2, 4, 8, 16, 32, ...).</p>

<p>The example above specifies keeping 4 of the most recent full backups (1 week interval) two full backups at 2 week intervals, and 3 full backups at 4 week intervals, eg:</p>

<pre><code>   full 0 19 weeks old   \
   full 1 15 weeks old    &gt;---  3 backups at 4 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>
   full 2 11 weeks old   /
   full 3  7 weeks old   \____  2 backups at 2 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>
   full 4  5 weeks old   /
   full 5  3 weeks old   \
   full 6  2 weeks old    \___  4 backups at 1 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>
   full 7  1 week old     /
   full 8  current       /</code></pre>

<p>On a given week the spacing might be less than shown as each backup ages through each expiry period. For example, one week later, a new full is completed and the oldest is deleted, giving:</p>

<pre><code>   full 0 16 weeks old   \
   full 1 12 weeks old    &gt;---  3 backups at 4 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>
   full 2  8 weeks old   /
   full 3  6 weeks old   \____  2 backups at 2 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>
   full 4  4 weeks old   /
   full 5  3 weeks old   \
   full 6  2 weeks old    \___  4 backups at 1 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>
   full 7  1 week old     /
   full 8  current       /</code></pre>

<p>You can specify 0 as a count (except in the first entry), and the array can be as long as you wish. For example:</p>

<pre><code>  <a href="#_conf_fullkeepcnt_">$Conf{FullKeepCnt}</a> = [4, 0, 4, 0, 0, 2];</code></pre>

<p>This will keep 10 full dumps, 4 most recent at 1 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>, followed by 4 at an interval of 4 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a> (approx 1 month apart), and then 2 at an interval of 32 * <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a> (approx 7-8 months apart).</p>

<p>Example: these two settings are equivalent and both keep just the four most recent full dumps:</p>

<pre><code>   <a href="#_conf_fullkeepcnt_">$Conf{FullKeepCnt}</a> = 4;
   <a href="#_conf_fullkeepcnt_">$Conf{FullKeepCnt}</a> = [4];</code></pre>

</dd>
<dt id="Conf-FullKeepCntMin-1"><a href="#_conf_fullkeepcntmin_">$Conf{FullKeepCntMin}</a> = 1;</dt>
<dd>

</dd>
<dt id="Conf-FullAgeMax-90"><a href="#_conf_fullagemax_">$Conf{FullAgeMax}</a> = 90;</dt>
<dd>

<p>Very old full backups are removed after <a href="#_conf_fullagemax_">$Conf{FullAgeMax}</a> days. However, we keep at least <a href="#_conf_fullkeepcntmin_">$Conf{FullKeepCntMin}</a> full backups no matter how old they are.</p>

<p>Note that <a href="#_conf_fullagemax_">$Conf{FullAgeMax}</a> will be increased to <a href="#_conf_fullkeepcnt_">$Conf{FullKeepCnt}</a> times <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a> if <a href="#_conf_fullkeepcnt_">$Conf{FullKeepCnt}</a> specifies enough full backups to exceed <a href="#_conf_fullagemax_">$Conf{FullAgeMax}</a>.</p>

</dd>
<dt id="Conf-IncrKeepCnt-6"><a href="#_conf_incrkeepcnt_">$Conf{IncrKeepCnt}</a> = 6;</dt>
<dd>

<p>Number of incremental backups to keep. Must be &gt;= 1.</p>

<p>In the steady state, each time an incr backup completes successfully the oldest one is removed. If this number is decreased, the extra old backups will be removed.</p>

</dd>
<dt id="Conf-IncrKeepCntMin-1"><a href="#_conf_incrkeepcntmin_">$Conf{IncrKeepCntMin}</a> = 1;</dt>
<dd>

</dd>
<dt id="Conf-IncrAgeMax-30"><a href="#_conf_incragemax_">$Conf{IncrAgeMax}</a> = 30;</dt>
<dd>

<p>Very old incremental backups are removed after <a href="#_conf_incragemax_">$Conf{IncrAgeMax}</a> days. However, we keep at least <a href="#_conf_incrkeepcntmin_">$Conf{IncrKeepCntMin}</a> incremental backups no matter how old they are.</p>

</dd>
<dt id="Conf-IncrLevels-1"><a href="#_conf_incrlevels_">$Conf{IncrLevels}</a> = [1];</dt>
<dd>

<p>Level of each incremental. &quot;Level&quot; follows the terminology of dump(1). A full backup has level 0. A new incremental of level N will backup all files that have changed since the most recent backup of a lower level.</p>

<p>The entries of <a href="#_conf_incrlevels_">$Conf{IncrLevels}</a> apply in order to each incremental after each full backup. It wraps around until the next full backup. For example, these two settings have the same effect:</p>

<pre><code>      <a href="#_conf_incrlevels_">$Conf{IncrLevels}</a> = [1, 2, 3];
      <a href="#_conf_incrlevels_">$Conf{IncrLevels}</a> = [1, 2, 3, 1, 2, 3];</code></pre>

<p>This means the 1st and 4th incrementals (level 1) go all the way back to the full. The 2nd and 3rd (and 5th and 6th) backups just go back to the immediate preceeding incremental.</p>

<p>Specifying a sequence of multi-level incrementals will usually mean more than <a href="#_conf_incrkeepcnt_">$Conf{IncrKeepCnt}</a> incrementals will need to be kept, since lower level incrementals are needed to merge a complete view of a backup. For example, with</p>

<pre><code>      <a href="#_conf_fullperiod_">$Conf{FullPeriod}</a>  = 7;
      <a href="#_conf_incrperiod_">$Conf{IncrPeriod}</a>  = 1;
      <a href="#_conf_incrkeepcnt_">$Conf{IncrKeepCnt}</a> = 6;
      <a href="#_conf_incrlevels_">$Conf{IncrLevels}</a>  = [1, 2, 3, 4, 5, 6];</code></pre>

<p>there will be up to 11 incrementals in this case:</p>

<pre><code>      backup #0  (full, level 0, oldest)
      backup #1  (incr, level 1)
      backup #2  (incr, level 2)
      backup #3  (incr, level 3)
      backup #4  (incr, level 4)
      backup #5  (incr, level 5)
      backup #6  (incr, level 6)
      backup #7  (full, level 0)
      backup #8  (incr, level 1)
      backup #9  (incr, level 2)
      backup #10 (incr, level 3)
      backup #11 (incr, level 4)
      backup #12 (incr, level 5, newest)</code></pre>

<p>Backup #1 (the oldest level 1 incremental) can&#39;t be deleted since backups 2..6 depend on it. Those 6 incrementals can&#39;t all be deleted since that would only leave 5 (#8..12). When the next incremental happens (level 6), the complete set of 6 older incrementals (#1..6) will be deleted, since that maintains the required number (<a href="#_conf_incrkeepcnt_">$Conf{IncrKeepCnt}</a>) of incrementals. This situation is reduced if you set shorter chains of multi-level incrementals, eg:</p>

<pre><code>      <a href="#_conf_incrlevels_">$Conf{IncrLevels}</a>  = [1, 2, 3];</code></pre>

<p>would only have up to 2 extra incremenals before all 3 are deleted.</p>

<p>BackupAFS as usual merges the full and the sequence of incrementals together so each incremental can be browsed and restored as though it is a complete backup. If you specify a long chain of incrementals then more backups need to be merged when browsing, restoring, or getting the starting point for rsync backups. In the example above (levels 1..6), browing backup #6 requires 7 different backups (#0..6) to be merged.</p>

<p>Because of this merging and the additional incrementals that need to be kept, it is recommended that some level 1 incrementals be included in <a href="#_conf_incrlevels_">$Conf{IncrLevels}</a>.</p>

<p>Prior to version 3.0 incrementals were always level 1, meaning each incremental backed up all the files that changed since the last full.</p>

</dd>
<dt id="Conf-BackupsDisable-0"><a href="#_conf_backupsdisable_">$Conf{BackupsDisable}</a> = 0;</dt>
<dd>

<p>Disable all full and incremental backups. These settings are useful for a client that is no longer being backed up (eg: a retired machine), but you wish to keep the last backups available for browsing or restoring to other machines.</p>

<p>There are three values for <a href="#_conf_backupsdisable_">$Conf{BackupsDisable}</a>:</p>

<pre><code>  0    Backups are enabled.

  1    Don&#39;t do any regular backups on this client. Manually
       requested backups (via the CGI interface) will still occur.

  2    Don&#39;t do any backups on this client. Manually requested
       backups (via the CGI interface) will be ignored.</code></pre>

</dd>
<dt id="Conf-RestoreInfoKeepCnt-10"><a href="#_conf_restoreinfokeepcnt_">$Conf{RestoreInfoKeepCnt}</a> = 10;</dt>
<dd>

<p>Number of restore logs to keep. BackupAFS remembers information about each restore request. This number per client will be kept around before the oldest ones are pruned.</p>

<p>Note: files/dirs downloaded via the browser don&#39;t count as restores. Only the first restore option (where the volumes are restored to AFS) count as restores that are logged.</p>

</dd>
<dt id="Conf-BlackoutPeriods"><a href="#_conf_blackoutperiods_">$Conf{BlackoutPeriods}</a> = [ ... ];</dt>
<dd>

<p>One or more blackout periods can be specified. If a client is subject to blackout then no regular (non-manual) backups will be started during any of these periods (already-running dumps will not be interruped, however). hourBegin and hourEnd specify hours from midnight and weekDays is a list of days of the week where 0 is Sunday, 1 is Monday etc.</p>

<p>For example:</p>

<pre><code>   <a href="#_conf_blackoutperiods_">$Conf{BlackoutPeriods}</a> = [
        {
            hourBegin =&gt;  7.0,
            hourEnd   =&gt; 19.5,
            weekDays  =&gt; [1, 2, 3, 4, 5],
        },
   ];</code></pre>

<p>specifies one blackout period from 7:00am to 7:30pm local time on Mon-Fri.</p>

<p>The blackout period can also span midnight by setting hourBegin &gt; hourEnd, eg:</p>

<pre><code>   <a href="#_conf_blackoutperiods_">$Conf{BlackoutPeriods}</a> = [
        {
            hourBegin =&gt;  7.0,
            hourEnd   =&gt; 19.5,
            weekDays  =&gt; [1, 2, 3, 4, 5],
        },
        {
            hourBegin =&gt; 23,
            hourEnd   =&gt;  5,
            weekDays  =&gt; [5, 6],
        },
   ];</code></pre>

<p>This specifies one blackout period from 7:00am to 7:30pm local time on Mon-Fri, and a second period from 11pm to 5am on Friday and Saturday night.</p>

</dd>
<dt id="Conf-BackupZeroFilesIsFatal-1"><a href="#_conf_backupzerofilesisfatal_">$Conf{BackupZeroFilesIsFatal}</a> = 1;</dt>
<dd>

<p>A backup of a share that has zero files is considered fatal. This is used to catch miscellaneous Xfer errors that result in no files being backed up. If you have shares that might be empty (and therefore an empty backup is valid) you should set this flag to 0. BackupAFS does not currently use this setting.</p>

</dd>
</dl>

<h2 id="How-to-backup-a-VolumeSet">How to backup a VolumeSet</h2>

<dl>

<dt id="Conf-XferMethod-vos"><a href="#_conf_xfermethod_">$Conf{XferMethod}</a> = &#39;vos&#39;;</dt>
<dd>

<p>What transport method to use to backup each volset. Currently there is only one valid XferMethod in BackupAFS:</p>

<pre><code>  - &#39;vos&#39;:     backup and restore via AFS &#39;vos dump&#39; and &#39;vos restore&#39;.</code></pre>

</dd>
<dt id="Conf-XferLogLevel-1"><a href="#_conf_xferloglevel_">$Conf{XferLogLevel}</a> = 1;</dt>
<dd>

<p>Level of verbosity in Xfer log files. 0 means be quiet, 1 will give will give one line per file, 2 will also show skipped files on incrementals, higher values give more output.</p>

</dd>
<dt id="Conf-PingPath"><a href="#_conf_pingpath_">$Conf{PingPath}</a> = &#39;&#39;;</dt>
<dd>

<p>Full path to the ping command. Security caution: normal users should not be allowed to write to this file or directory.</p>

<p>If you want to disable ping checking, set this to some program that exits with 0 status, eg:</p>

<pre><code>    <a href="#_conf_pingpath_">$Conf{PingPath}</a> = &#39;/bin/echo&#39;;</code></pre>

</dd>
<dt id="Conf-PingCmd-pingPath--c-1-host"><a href="#_conf_pingcmd_">$Conf{PingCmd}</a> = &#39;$pingPath -c 1 $host&#39;;</dt>
<dd>

<p>Ping command. The following variables are substituted at run-time:</p>

<pre><code>  $pingPath      path to ping (<a href="#_conf_pingpath_">$Conf{PingPath}</a>)
  $host          host name</code></pre>

<p>Wade Brown reports that on solaris 2.6 and 2.7 ping -s returns the wrong exit status (0 even on failure). Replace with &quot;ping $host 1&quot;, which gets the correct exit status but we don&#39;t get the round-trip time.</p>

<p>Note: all Cmds are executed directly without a shell, so the prog name needs to be a full path and you can&#39;t include shell syntax like redirection and pipes; put that in a script if you need it.</p>

</dd>
<dt id="Conf-PingMaxMsec-20"><a href="#_conf_pingmaxmsec_">$Conf{PingMaxMsec}</a> = 20;</dt>
<dd>

<p>Maximum round-trip ping time in milliseconds.</p>

</dd>
<dt id="Conf-CompressLevel-0"><a href="#_conf_compresslevel_">$Conf{CompressLevel}</a> = 0;</dt>
<dd>

<p>Compression level to use on files. 0 means no compression. Compression levels can be from 1 (least cpu time, worst compression) to 9 (most cpu time, better compression). The recommended value is 3 or 4. Changing to 5, for example, will take maybe 20% more cpu time and will get another 2-3% additional compression. See the gzip documentation for more information about compression levels.</p>

<p>Note: compression requires either &#39;gzip&#39; or &#39;pigz&#39; be installed and executable by the backupafs user. If the requested binary can&#39;t be found then <a href="#_conf_compresslevel_">$Conf{CompressLevel}</a> is forced to 0 (compression off).</p>

</dd>
<dt id="Conf-ClientTimeout-72000"><a href="#_conf_clienttimeout_">$Conf{ClientTimeout}</a> = 72000;</dt>
<dd>

<p>Timeout in seconds when listening for the transport program&#39;s (vos) stdout. If no output is received during this time, then it is assumed that something has wedged during a backup, and the backup is terminated.</p>

<p>Despite the name, this parameter sets the timeout for all transport methods (vos).</p>

</dd>
<dt id="Conf-MaxOldPerPCLogFiles-12"><a href="#_conf_maxoldperpclogfiles_">$Conf{MaxOldPerPCLogFiles}</a> = 12;</dt>
<dd>

<p>Maximum number of log files we keep around in each PC&#39;s directory (ie: volsets/$volset). These files are aged monthly. A setting of 12 means there will be at most the files LOG, LOG.0, LOG.1, ... LOG.11 in the volsets/$volset directory (ie: about a years worth). (Except this month&#39;s LOG, these files will have a .z extension if compression is on).</p>

<p>If you decrease this number after BackupAFS has been running for a while you will have to manually remove the older log files.</p>

</dd>
<dt id="Conf-DumpPreUserCmd-undef"><a href="#_conf_dumppreusercmd_">$Conf{DumpPreUserCmd}</a> = undef;</dt>
<dd>

</dd>
<dt id="Conf-DumpPostUserCmd-undef"><a href="#_conf_dumppostusercmd_">$Conf{DumpPostUserCmd}</a> = undef;</dt>
<dd>

</dd>
<dt id="Conf-DumpPreShareCmd-undef"><a href="#_conf_dumppresharecmd_">$Conf{DumpPreShareCmd}</a> = undef;</dt>
<dd>

</dd>
<dt id="Conf-DumpPostShareCmd-undef"><a href="#_conf_dumppostsharecmd_">$Conf{DumpPostShareCmd}</a> = undef;</dt>
<dd>

</dd>
<dt id="Conf-RestorePreUserCmd-undef"><a href="#_conf_restorepreusercmd_">$Conf{RestorePreUserCmd}</a> = undef;</dt>
<dd>

</dd>
<dt id="Conf-RestorePostUserCmd-undef"><a href="#_conf_restorepostusercmd_">$Conf{RestorePostUserCmd}</a> = undef;</dt>
<dd>

<p>Optional commands to run before and after dumps and restores, and also before and after each volume of a dump.</p>

<p>Stdout from these commands will be written to the Xfer (or Restore) log file. One example of using these commands would be to shut down and restart a database process.</p>

<p><b>These commands have not been tested with BackupAFS; they are a holdover from BackupPC.</b></p>

<pre><code>   <a href="#_conf_dumppreusercmd_">$Conf{DumpPreUserCmd}</a> = &#39;$sshPath -q -x -l root $host /usr/bin/dumpMysql&#39;;</code></pre>

<p>The following variable substitutions are made at run time for <a href="#_conf_dumppreusercmd_">$Conf{DumpPreUserCmd}</a>, <a href="#_conf_dumppostusercmd_">$Conf{DumpPostUserCmd}</a>, <a href="#_conf_dumppresharecmd_">$Conf{DumpPreShareCmd}</a> and <a href="#_conf_dumppostsharecmd_">$Conf{DumpPostShareCmd}</a>:</p>

<pre><code>       $type         type of dump (incr or full)
       $xferOK       1 if the dump succeeded, 0 if it didn&#39;t
       $client       client name being backed up
       $host         host name (could be different from client name if
                                <a href="#_conf_clientnamealias_">$Conf{ClientNameAlias}</a> is set)
       $hostIP       IP address of host
       $user         user name from the VolumeSet-List file
       $moreUsers    list of additional users from the VolumeSet-List file
       $share        the first share name (or current share for
                       <a href="#_conf_dumppresharecmd_">$Conf{DumpPreShareCmd}</a> and <a href="#_conf_dumppostsharecmd_">$Conf{DumpPostShareCmd}</a>)
       $shares       list of all the share names
       $XferMethod   value of <a href="#_conf_xfermethod_">$Conf{XferMethod}</a> (eg: tar, rsync, smb)
       $sshPath      value of <a href="#_conf_sshpath_">$Conf{SshPath}</a>,
       $cmdType      set to DumpPreUserCmd or DumpPostUserCmd</code></pre>

<p>The following variable substitutions are made at run time for <a href="#_conf_restorepreusercmd_">$Conf{RestorePreUserCmd}</a> and <a href="#_conf_restorepostusercmd_">$Conf{RestorePostUserCmd}</a>:</p>

<pre><code>       $client       client name being backed up
       $xferOK       1 if the restore succeeded, 0 if it didn&#39;t
       $host         host name (could be different from client name if
                                <a href="#_conf_clientnamealias_">$Conf{ClientNameAlias}</a> is set)
       $hostIP       IP address of host
       $user         user name from the VolumeSet-List file
       $moreUsers    list of additional users from the VolumeSet-List file
       $share        the first share name
       $XferMethod   value of <a href="#_conf_xfermethod_">$Conf{XferMethod}</a> (eg: tar, rsync, smb)
       $sshPath      value of <a href="#_conf_sshpath_">$Conf{SshPath}</a>,
       $type         set to &quot;restore&quot;
       $bkupSrcHost  host name of the restore source
       $bkupSrcShare share name of the restore source
       $bkupSrcNum   backup number of the restore source
       $pathHdrSrc   common starting path of restore source
       $pathHdrDest  common starting path of destination
       $fileList     list of files being restored
       $cmdType      set to RestorePreUserCmd or RestorePostUserCmd</code></pre>

<p>Note: all Cmds are executed directly without a shell, so the prog name needs to be a full path and you can&#39;t include shell syntax like redirection and pipes; put that in a script if you need it.</p>

</dd>
<dt id="Conf-UserCmdCheckStatus-0"><a href="#_conf_usercmdcheckstatus_">$Conf{UserCmdCheckStatus}</a> = 0;</dt>
<dd>

<p>Whether the exit status of each PreUserCmd and PostUserCmd is checked.</p>

<p>If set and the Dump/Restore/Archive Pre/Post UserCmd returns a non-zero exit status then the dump/restore/archive is aborted. To maintain backward compatibility (where the exit status in early versions was always ignored), this flag defaults to 0.</p>

<p>If this flag is set and the Dump/Restore/Archive PreUserCmd fails then the matching Dump/Restore/Archive PostUserCmd is not executed. If DumpPreShareCmd returns a non-exit status, then DumpPostShareCmd is not executed, but the DumpPostUserCmd is still run (since DumpPreUserCmd must have previously succeeded).</p>

<p>An example of a DumpPreUserCmd that might fail is a script that snapshots or dumps a database which fails because of some database error.</p>

</dd>
</dl>

<h2 id="Email-reminders-status-and-messages">Email reminders, status and messages</h2>

<dl>

<dt id="Conf-SendmailPath"><a href="#_conf_sendmailpath_">$Conf{SendmailPath}</a> = &#39;&#39;;</dt>
<dd>

<p>Full path to the sendmail command. Security caution: normal users should not allowed to write to this file or directory.</p>

</dd>
<dt id="Conf-EMailNotifyMinDays-2.5"><a href="#_conf_emailnotifymindays_">$Conf{EMailNotifyMinDays}</a> = 2.5;</dt>
<dd>

<p>Minimum period between consecutive emails to a single user. This tries to keep annoying email to users to a reasonable level. Email checks are done nightly, so this number is effectively rounded up (ie: 2.5 means a user will never receive email more than once every 3 days).</p>

</dd>
<dt id="Conf-EMailFromUserName"><a href="#_conf_emailfromusername_">$Conf{EMailFromUserName}</a> = &#39;&#39;;</dt>
<dd>

<p>Name to use as the &quot;from&quot; name for email. Depending upon your mail handler this is either a plain name (eg: &quot;admin&quot;) or a fully-qualified name (eg: &quot;admin@mydomain.com&quot;).</p>

</dd>
<dt id="Conf-EMailAdminUserName"><a href="#_conf_emailadminusername_">$Conf{EMailAdminUserName}</a> = &#39;&#39;;</dt>
<dd>

<p>Destination address to an administrative user who will receive a nightly email with warnings and errors. If there are no warnings or errors then no email will be sent. Depending upon your mail handler this is either a plain name (eg: &quot;admin&quot;) or a fully-qualified name (eg: &quot;admin@mydomain.com&quot;).</p>

</dd>
<dt id="Conf-EMailUserDestDomain"><a href="#_conf_emailuserdestdomain_">$Conf{EMailUserDestDomain}</a> = &#39;&#39;;</dt>
<dd>

<p>Destination domain name for email sent to users. By default this is empty, meaning email is sent to plain, unqualified addresses. Otherwise, set it to the destintation domain, eg:</p>

<pre><code>   $Cong{EMailUserDestDomain} = &#39;@mydomain.com&#39;;</code></pre>

<p>With this setting user email will be set to &#39;user@mydomain.com&#39;.</p>

</dd>
<dt id="Conf-EMailNoBackupEverSubj-undef"><a href="#_conf_emailnobackupeversubj_">$Conf{EMailNoBackupEverSubj}</a> = undef;</dt>
<dd>

</dd>
<dt id="Conf-EMailNoBackupEverMesg-undef"><a href="#_conf_emailnobackupevermesg_">$Conf{EMailNoBackupEverMesg}</a> = undef;</dt>
<dd>

<p>This subject and message is sent to a user if their PC has never been backed up.</p>

<p>These values are language-dependent. The default versions can be found in the language file (eg: lib/BackupAFS/Lang/en.pm). If you need to change the message, copy it here and edit it, eg:</p>

<pre><code>  <a href="#_conf_emailnobackupevermesg_">$Conf{EMailNoBackupEverMesg}</a> = &lt;&lt;&#39;EOF&#39;;
  To: $user$domain
  cc:
  Subject: $subj

  Dear $userName,

  This is a site-specific email message.
  EOF</code></pre>

</dd>
<dt id="Conf-EMailNotifyOldBackupDays-7.0"><a href="#_conf_emailnotifyoldbackupdays_">$Conf{EMailNotifyOldBackupDays}</a> = 7.0;</dt>
<dd>

<p>How old the most recent backup has to be before notifying user. When there have been no backups in this number of days the user is sent an email.</p>

</dd>
<dt id="Conf-EMailNoBackupRecentSubj-undef"><a href="#_conf_emailnobackuprecentsubj_">$Conf{EMailNoBackupRecentSubj}</a> = undef;</dt>
<dd>

</dd>
<dt id="Conf-EMailNoBackupRecentMesg-undef"><a href="#_conf_emailnobackuprecentmesg_">$Conf{EMailNoBackupRecentMesg}</a> = undef;</dt>
<dd>

<p>This subject and message is sent to a user if their PC has not recently been backed up (ie: more than <a href="#_conf_emailnotifyoldbackupdays_">$Conf{EMailNotifyOldBackupDays}</a> days ago).</p>

<p>These values are language-dependent. The default versions can be found in the language file (eg: lib/BackupAFS/Lang/en.pm). If you need to change the message, copy it here and edit it, eg:</p>

<pre><code>  <a href="#_conf_emailnobackuprecentmesg_">$Conf{EMailNoBackupRecentMesg}</a> = &lt;&lt;&#39;EOF&#39;;
  To: $user$domain
  cc:
  Subject: $subj

  Dear $userName,

  This is a site-specific email message.
  EOF</code></pre>

</dd>
<dt id="Conf-EMailHeaders-EOF"><a href="#_conf_emailheaders_">$Conf{EMailHeaders}</a> = &lt;&lt;EOF;</dt>
<dd>

<p>Additional email headers. This sets to charset to utf8.</p>

</dd>
</dl>

<h2 id="CGI-user-interface-configuration-settings">CGI user interface configuration settings</h2>

<dl>

<dt id="Conf-CgiAdminUserGroup"><a href="#_conf_cgiadminusergroup_">$Conf{CgiAdminUserGroup}</a> = &#39;&#39;;</dt>
<dd>

</dd>
<dt id="Conf-CgiAdminUsers"><a href="#_conf_cgiadminusers_">$Conf{CgiAdminUsers}</a> = &#39;&#39;;</dt>
<dd>

<p>Normal users can only access information specific to their volset. They can start/stop/browse/restore backups.</p>

<p>Administrative users have full access to all VolumeSets , plus overall status and log information.</p>

<p>The administrative users are the union of the unix/linux group <a href="#_conf_cgiadminusergroup_">$Conf{CgiAdminUserGroup}</a> and the manual list of users, separated by spaces, in <a href="#_conf_cgiadminusers_">$Conf{CgiAdminUsers}</a>. If you don&#39;t want a group or manual list of users set the corresponding configuration setting to undef or an empty string.</p>

<p>If you want every user to have admin privileges (careful!), set <a href="#_conf_cgiadminusers_">$Conf{CgiAdminUsers}</a> = &#39;*&#39;.</p>

<p>Examples:</p>

<pre><code>   <a href="#_conf_cgiadminusergroup_">$Conf{CgiAdminUserGroup}</a> = &#39;admin&#39;;
   <a href="#_conf_cgiadminusers_">$Conf{CgiAdminUsers}</a>     = &#39;craig celia&#39;;
   --&gt; administrative users are the union of group admin, plus
     craig and celia.

   <a href="#_conf_cgiadminusergroup_">$Conf{CgiAdminUserGroup}</a> = &#39;&#39;;
   <a href="#_conf_cgiadminusers_">$Conf{CgiAdminUsers}</a>     = &#39;craig celia&#39;;
   --&gt; administrative users are only craig and celia&#39;.</code></pre>

</dd>
<dt id="Conf-CgiURL-undef"><a href="#_conf_cgiurl_">$Conf{CgiURL}</a> = undef;</dt>
<dd>

<p>URL of the BackupAFS_Admin CGI script. Used for email messages.</p>

</dd>
<dt id="Conf-Language-en"><a href="#_conf_language_">$Conf{Language}</a> = &#39;en&#39;;</dt>
<dd>

<p>Language to use. See lib/BackupAFS/Lang for the list of supported languages, which include English (en), French (fr), Spanish (es), German (de), Italian (it), Dutch (nl), Polish (pl), Portuguese Brazillian (pt_br) and Chinese (zh_CH).</p>

<p>Currently the Language setting applies to the CGI interface and email messages sent to users. Log files and other text are still in English.</p>

</dd>
<dt id="Conf-CgiUserHomePageCheck"><a href="#_conf_cgiuserhomepagecheck_">$Conf{CgiUserHomePageCheck}</a> = &#39;&#39;;</dt>
<dd>

</dd>
<dt id="Conf-CgiUserUrlCreate-mailto:-s"><a href="#_conf_cgiuserurlcreate_">$Conf{CgiUserUrlCreate}</a> = &#39;mailto:%s&#39;;</dt>
<dd>

<p>User names that are rendered by the CGI interface can be turned into links into their home page or other information about the user. To set this up you need to create two sprintf() strings, that each contain a single &#39;%s&#39; that will be replaced by the user name. The default is a mailto: link.</p>

<p><a href="#_conf_cgiuserhomepagecheck_">$Conf{CgiUserHomePageCheck}</a> should be an absolute file path that is used to check (via &quot;-f&quot;) that the user has a valid home page. Set this to undef or an empty string to turn off this check.</p>

<p><a href="#_conf_cgiuserurlcreate_">$Conf{CgiUserUrlCreate}</a> should be a full URL that points to the user&#39;s home page. Set this to undef or an empty string to turn off generation of URLs for user names.</p>

<p>Example:</p>

<pre><code>   <a href="#_conf_cgiuserhomepagecheck_">$Conf{CgiUserHomePageCheck}</a> = &#39;/var/www/html/users/%s.html&#39;;
   <a href="#_conf_cgiuserurlcreate_">$Conf{CgiUserUrlCreate}</a>     = &#39;http://myhost/users/%s.html&#39;;
   --&gt; if /var/www/html/users/craig.html exists, then &#39;craig&#39; will
     be rendered as a link to http://myhost/users/craig.html.</code></pre>

</dd>
<dt id="Conf-CgiDateFormatMMDD-1"><a href="#_conf_cgidateformatmmdd_">$Conf{CgiDateFormatMMDD}</a> = 1;</dt>
<dd>

<p>Date display format for CGI interface. A value of 1 uses US-style dates (MM/DD), a value of 2 uses full YYYY-MM-DD format, and zero for international dates (DD/MM).</p>

</dd>
<dt id="Conf-CgiSearchBoxEnable-1"><a href="#_conf_cgisearchboxenable_">$Conf{CgiSearchBoxEnable}</a> = 1;</dt>
<dd>

<p>Enable/disable the search box in the navigation bar.</p>

</dd>
<dt id="Conf-CgiNavBarLinks"><a href="#_conf_cginavbarlinks_">$Conf{CgiNavBarLinks}</a> = [ ... ];</dt>
<dd>

<p>Additional navigation bar links. These appear for both regular users and administrators. This is a list of hashes giving the link (URL) and the text (name) for the link. Specifying lname instead of name uses the language specific string (ie: $Lang-&gt;{lname}) instead of just literally displaying name.</p>

</dd>
<dt id="Conf-CgiStatusHilightColor"><a href="#_conf_cgistatushilightcolor_">$Conf{CgiStatusHilightColor}</a> = { ...</dt>
<dd>

<p>Hilight colors based on status that are used in the PC summary page.</p>

</dd>
<dt id="Conf-CgiHeaders-meta-http-equiv-pragma-content-no-cache"><a href="#_conf_cgiheaders_">$Conf{CgiHeaders}</a> = &#39;&lt;meta http-equiv=&quot;pragma&quot; content=&quot;no-cache&quot;&gt;&#39;;</dt>
<dd>

<p>Additional CGI header text.</p>

</dd>
<dt id="Conf-CgiImageDir"><a href="#_conf_cgiimagedir_">$Conf{CgiImageDir}</a> = &#39;&#39;;</dt>
<dd>

<p>Directory where images are stored. This directory should be below Apache&#39;s DocumentRoot. This value isn&#39;t used by BackupAFS but is used by configure.pl when you upgrade BackupAFS.</p>

<p>Example:</p>

<pre><code>    <a href="#_conf_cgiimagedir_">$Conf{CgiImageDir}</a> = &#39;/var/www/htdocs/BackupAFS&#39;;</code></pre>

</dd>
<dt id="Conf-CgiExt2ContentType"><a href="#_conf_cgiext2contenttype_">$Conf{CgiExt2ContentType}</a> = { };</dt>
<dd>

<p>Additional mappings of file name extenions to Content-Type for individual file restore. See $Ext2ContentType in BackupAFS_Admin for the default setting. You can add additional settings here, or override any default settings. Example:</p>

<pre><code>    <a href="#_conf_cgiext2contenttype_">$Conf{CgiExt2ContentType}</a> = {
                &#39;pl&#39;  =&gt; &#39;text/plain&#39;,
         };</code></pre>

</dd>
<dt id="Conf-CgiImageDirURL"><a href="#_conf_cgiimagedirurl_">$Conf{CgiImageDirURL}</a> = &#39;&#39;;</dt>
<dd>

<p>URL (without the leading http://host) for BackupAFS&#39;s image directory. The CGI script uses this value to serve up image files.</p>

<p>Example:</p>

<pre><code>    <a href="#_conf_cgiimagedirurl_">$Conf{CgiImageDirURL}</a> = &#39;/BackupAFS&#39;;</code></pre>

</dd>
<dt id="Conf-CgiCSSFile-BackupAFS_stnd.css"><a href="#_conf_cgicssfile_">$Conf{CgiCSSFile}</a> = &#39;BackupAFS_stnd.css&#39;;</dt>
<dd>

<p>CSS stylesheet &quot;skin&quot; for the CGI interface. It is stored in the <a href="#_conf_cgiimagedir_">$Conf{CgiImageDir}</a> directory and accessed via the <a href="#_conf_cgiimagedirurl_">$Conf{CgiImageDirURL}</a> URL.</p>

<p>For BackupAFS v3.x several color, layout and font changes were made. The previous v2.x version is available as BackupAFS_stnd_orig.css, so if you prefer the old skin, change this to BackupAFS_stnd_orig.css.</p>

</dd>
<dt id="Conf-CgiUserConfigEditEnable-1"><a href="#_conf_cgiuserconfigeditenable_">$Conf{CgiUserConfigEditEnable}</a> = 1;</dt>
<dd>

<p>Whether the user is allowed to edit their per-PC config.</p>

</dd>
<dt id="Conf-CgiUserConfigEdit"><a href="#_conf_cgiuserconfigedit_">$Conf{CgiUserConfigEdit}</a> = { ...</dt>
<dd>

<p>Which per-volset config variables a non-admin user is allowed to edit. Admin users can edit all per-volset config variables, even if disabled in this list.</p>

<p>SECURITY WARNING: Do not let users edit any of the Cmd config variables! That&#39;s because a user could set a Cmd to a shell script of their choice and it will be run as the BackupAFS user. That script could do all sorts of bad things.</p>

</dd>
</dl>

<a href="#_podtop_"><h1 id="Migrating-from-BackupPC4AFS">Migrating from BackupPC4AFS</h1></a>

<p>While both software applications are based on BackupPC, there are several notable functional differences between BackupPC4AFS and BackupAFS.</p>

<h2 id="Migrating-VolumeSets">Migrating VolumeSets</h2>

<p>If you have not been using the AFS &quot;backup&quot; database (whether using butc, BackupPC4AFS, etc), you may safely skip this section.</p>

<p>BackupPC4AFS stored the definition of exactly which volumes are included in a specific volumeset in the AFS backup database. BackupAFS stores this definition in the VolumeSet-List file. The exact format of the VolumeSet-List file is covered in Step 6: Setting up the VolumeSet-List file portion of the installation instructions.</p>

<p>BackupPC4AFS prefixed all AFS volume sets with &quot;afs_&quot;, which was used internally to indicate to BackupPC4AFS that a specific volumeset (host) actually represented a set of volumes to dump. <b>BackupAFS does NOT do this. The name of the volumeset is used as recorded in the VolumeSet-List file.</b></p>

<p>To facilitate the migration of volumesets and their definitions (volume entries) from the AFS database to the BackupAFS VolumeSet-List file, a migration script, BackupAFS_migrate_populate_VolSet-List is included with the distribution.</p>

<p>&quot;BackupAFS_migrate_populate_VolSet-List&quot; takes no arguments. It queries the AFS backup database and outputs its best guess at correct VolumeSet names and corresponding volume entries on STDOUT. It does expect to be able to query the backup database using the -localauth option, therefore it should be executed after the cell&#39;s KeyFile is already installed on the BackupAFS server.</p>

<p>Limitations:</p>

<ul>

<li><p>The number of volume entries in BackupAFS is limited to 5. This means that existing volume sets, stored in the AFS backup database, which have more than 5 volume entries, will be truncated to 5 entries. It is advised to check the output and add any missing entries to a new VolumeSet in BackupAFS (either via the CGI or by editing the VolumeSet-List file</p>

</li>
<li><p>The AFS backup database has no notion of user ownership of volume sets. Therefore the contents of the &quot;users&quot; and &quot;moreUsers&quot; fields will be blank. These can be set after migration, if desired.</p>

</li>
<li><p>The migration script must be executed as a user with permissions to run &quot;backup -localauth&quot; and permission to write to the /etc/BackupAFS/VolSet-List file. This is usually backup or root.</p>

</li>
</ul>

<p>It is recommended to run a test first:</p>

<pre><code>    cd /etc/BackupAFS
    __INSTALLDIR__/bin/BackupAFS_migrate_populate_VolSet-List
    backup: waiting for job termination
    test_all:::.*:.*:test\..*\.backup::::::::::::</code></pre>

<p>Note that the line beginning &quot;backup: waiting...&quot; is on STDERR, not STDOUT. If the output looks amenable, add it to the VolumeSet-List file:</p>

<pre><code>    cd /etc/BackupAFS
    __INSTALLDIR__/bin/BackupAFS_migrate_populate_VolSet-List &gt;&gt; VolumeSet-List
    backup: waiting for job termination</code></pre>

<p>Any volsets with more than 5 volume entries will be omitted and you will be warned by BackupAFS_migrate_populate_VolSet-List. Make sure to check the results for sanity to make sure it looks correct for your cell.</p>

<pre><code>    cd /etc/BackupAFS
    __INSTALLDIR__/bin/BackupAFS_migrate_populate_VolSet-List &gt;&gt; VolumeSet-List
    backup: waiting for job termination
    test1 has more than 5 volentries. Omitting &quot;    Entry   6: server .*, partition .*, volumes: e30.*\.backup&quot;
    test1 has more than 5 volentries. Omitting &quot;    Entry   7: server .*, partition .*, volumes: d70.*\.backup&quot;
    test1 has more than 5 volentries. Omitting &quot;    Entry   8: server .*, partition .*, volumes: oar.*\.backup&quot;
    test1 has more than 5 volentries. Omitting &quot;    Entry   9: server .*, partition .*, volumes: r90.*\.backup&quot;
    test2 has more than 5 volentries. Omitting &quot;    Entry   6: server .*, partition .*, volumes: d70\..*\.backup&quot;</code></pre>

<h2 id="Unmangling-the-Existing-Backups">Unmangling the Existing Backups</h2>

<p>If you are not migrating from BackupPC4AFS, you may safely skip this section.</p>

<p>BackupPC4AFS stored its dump data in files which had &quot;mangled&quot; names. Name mangling is a concept used by BackupPC (the product on which BackupPC4AFS and BackupAFS are based) to avoid namespace collisions and to allow it to store files&#39; metadata separately from the file itself. BackupPC4AFS went with the flow and mangled filenames because the CGI interface understood mangled names by default.</p>

<p>BackupAFS does not mangle file names. Therefore it is recommended to unmangle the existing backups to prevent confusion.</p>

<p>Additionally, the default backup directory (datadir) in BackupPC4AFS is <a href="#_conf_topdir_">$Conf{TopDir}</a>/pc. For example, /srv/BackupPC/pc. BackupAFS stores its volume backups in <a href="#_conf_topdir_">$Conf{TopDir}</a>/volsets. For example, __TOPDIR__/volsets.</p>

<p>As mentioned in the section above, BackupPC4AFS prefixed all AFS volume sets with &quot;afs_&quot;, which was used internally to indicate to BackupPC4AFS that a specific volumeset (host) actually represented a set of volumes to dump. <b>BackupAFS does NOT do this. The name of the volumeset is used as recorded in the VolumeSet-List file.</b> BackupAFS_migrate_unmangle_datadir renames the data directories to remove the &quot;afs_&quot; prefix. ($volsetname=~s^afs_//;)</p>

<p>To ease migration, BackupAFS comes with a script, BackupAFS_migrate_unmangle_datadir to move any existing backups from the BackupPC4AFS directory structure and names to that expected by BackupAFS.</p>

<p>BackupAFS_migrate_unmangle_datadir takes one argument, the defined TopDir, passed in as --topdir=path. When properly executed, the script takes no action itself. It merely outputs, to STDOUT, a bourne-shell script which contains the actions that are necessary. This gives the admin a chance to review the output for sanity prior to execution.</p>

<pre><code>    __INSTALLDIR__/bin/BackupAFS_migrate_unmangle_datadir --topdir=__INSTALLDIR__ &gt;&gt; /tmp/unmangle.sh
    less /tmp/unmangle.sh                                                # Please review for correctness.
    chmod u+rx /tmp/unmangle.sh
    /tmp/unmangle.sh                                                     # This may take some time to execute.</code></pre>

<h2 id="Compression-of-Existing-Backups">Compression of Existing Backups</h2>

<p>If you are not migrating from BackupPC4AFS, you may safely skip this section.</p>

<p>BackupPC4AFS stored its dump data in files which were uncompressed. If available and requested, BackupAFS can compress dumps immediately after they occur in order to save disk space.</p>

<p>The backup and restore routines know how to handle both compressed and uncompressed dumps, so it is not necessary to compress existing dumps, however it is recommended and can save considerable space (35% or more is not uncommon).</p>

<p>If you do decide to compress existing dumps, please do so as instructed here. Manually compressing files outside of these guidelines will not store the compression statistics, and BackupAFS will not be able to accurately report compression savings.</p>

<p>In order to facilitate compression of existing dumps, a script named BackupAFS_migrate_compress_volsets is included in the distribution. This script may be used to schedule the compression of an entire data directory or a single volumeset, to give the administrator flexibility. Compressing the entire data directory may be very time consuming, depending on the volume of existing data and the speed and quantity of processors.</p>

<p>A real-world example: compressing 9.7TB of data (375 full dumps, 6000 incrementals), on a server with (8) 2.66GHz X5355 Xeon processors (using pigz for parallel compression threads) and 4GB of RAM took approximately 20 hours.</p>

<p>The BackupAFS_migrate_compress_volsets script takes either 2 or 3 arguments. --datadir= and --backupuser= are mandatory. --volset is optional, and if specified will operate on only the specified volumeset. If --volset is omitted, then all volumesets will be processed.</p>

<p>The action of the script is to locate all volume dump files (.vdmp files) for each volumeset and add them to a &quot;NewFileList.backupnumber&quot; file inside the volset&#39;s directory. Once the &quot;NewFileList&quot; file is constructed, BackupAFS can be requested to perform the compression immediately or it will be performed during the next regularly-scheduled wakeup period (along with any files backed up during that wakeup period).</p>

<p>To compress all backups at once, an admin might do:</p>

<pre><code>    __INSTALLDIR__/bin/BackupAFS_migrate_compress_volsets --datadir=__TOPDIR__ --backupuser=backup
    # Output snipped for documentation purposes, but each file found is echoed on STDOUT</code></pre>

<p>To compress all backups for ONLY ONE volume set, an admin might do:</p>

<pre><code>    __INSTALLDIR__/bin/BackupAFS_migrate_compress_volsets --datadir=__TOPDIR__ --backupuser=backup --volset=test1
    # Output snipped for documentation purposes, but each file found is echoed on STDOUT</code></pre>

<p>After performing either of the above steps, to request BackupAFS perform a compression for a given volset immediately, issue the following command, substituting the name of one of your volumesets in the place of &quot;test1&quot; and substituting in the name of the backup user if it is not backup.</p>

<pre><code>    su -c &quot;__INSTALLDIR__/bin/BackupAFS_serverMesg compress test1&quot; backup
    Got reply: ok</code></pre>

<p>The above command may be repeated for each volumeset. Additional compressions will be queued since only one compress operation may occur at any given time. Compressions scheduled via the &quot;BackupAFS_serverMesg compress&quot; method will show up in the CGI (in Status and Current queues) and statistics for it will be recorded in each volumeset&#39;s &quot;backups&quot; file.</p>

<p>NOTE that the maximum number of pending jobs that may exist is defined by <a href="#_conf_maxpendingcmds_">$Conf{MaxPendingCmds}</a>. If the number of pending jobs equals or exceeds the value defined there, no new dumps will occur until the number of pending jobs decreases. Therefore you may wish to temporarily increase this number to a very high number if you wish to allow compressions to occur without hampering backups. This would be useful if the expected duration of compression is greater than your backup interval.</p>

<a href="#_podtop_"><h1 id="Version-Numbers">Version Numbers</h1></a>

<p>Starting with v1.0.0 BackupAFS uses a X.Y.Z version numbering system. The first digit is for major new releases, the middle digit is for significant feature releases and improvements (most of the releases will likely be in this category), and the last digit is for bug fixes. You should think of the old 1.00, 1.01, 1.02 and 1.03 as 1.0.0, 1.1.0, 1.2.0 and 1.3.0.</p>

<p>Additionally, patches might be made available. A patched version number is of the form X.Y.ZplN (eg: 2.1.0pl2), where N is the patch level.</p>

<a href="#_podtop_"><h1 id="Author">Author</h1></a>

<p>Craig Barratt &lt;cbarratt@users.sourceforge.net&gt; wrote the original BackupPC documentation on which this documentation is based. Any and all subsequent modifications to this document, especially the AFS-specific parts, were written by Stephen Joyce &lt;stephen@email.unc.edu&gt;. Please do not send BackupAFS questions to Mr. Barratt.</p>

<p>See <a href="http://backupafs.sourceforge.net">http://backupafs.sourceforge.net</a>.</p>

<a href="#_podtop_"><h1 id="Copyright">Copyright</h1></a>

<p>Copyright (C) 2007-2010 Stephen Joyce</p>

<p>Copyright for portions originally from BackupPC documentation remain copyright (C) 2001-2009 Craig Barratt.</p>

<a href="#_podtop_"><h1 id="Credits">Credits</h1></a>

<p>Craig Barratt is the primary author and developer of BackupPC, the application on which BackupAFS is largely based. Without BackupPC, and the fact that it is GPL&#39;ed software, BackupAFS would not exist.</p>

<p>Ryan Kucera contributed the directory navigation code and images for BackupPC v1.5.0. He contributed the first skeleton of BackupPC_restore. He also added a significant revision to the CGI interface, including CSS tags, in BackupPC v2.1.0.</p>

<p>Rich Duzenbury wrote the RSS feed option for the CGI interface.</p>

<p>Jono Woodhouse from CapeSoft Software (www.capesoft.com) provided a new CSS skin for BackupPC v3.0.0 with several layout improvements. Sean Cameron (also from CapeSoft) designed new and more compact file icons for BackupPC v3.0.0.</p>

<p>Your name could appear here in the next version!</p>

<a href="#_podtop_"><h1 id="License">License</h1></a>

<p>BackupAFS is based on BackupPC. BackupPC is (C) 2001-2009 Craig Barratt. BackupPC is free software, available under the GNU AGPL v3 (ONLY; No other version).</p>

<p>All portions of BackupAFS not (C) Craig Barratt are (C) 2007-2010 Stephen Joyce.</p>

<p><b>This program (BackupAFS) is released under the GNU AGPL v3 license (no other versions).</b></p>

<p>This program is free software: you can redistribute it and/or modify it under the terms of <b>VERSION 3</b> of the GNU Affero General Public License as published by the Free Software Foundation.</p>

<p>This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.</p>

<p>You should have received a copy of the GNU Affero General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;.</p>

<table border="0" width="100%" cellspacing="0" cellpadding="3">
<tr><td class="_podblock_" style="background-color: #cccccc" valign="middle">
<big><strong><span class="_podblock_">&nbsp;BackupAFS</span></strong></big>
</td></tr>
</table>

</body>

</html>


